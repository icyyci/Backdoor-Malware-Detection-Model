{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Backdoor Malware Analysis.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"vmKCBPPiOB7D"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0HSMOtS2_jC6","executionInfo":{"status":"ok","timestamp":1648719284676,"user_tz":-480,"elapsed":23499,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}},"outputId":"46b2200b-c316-4dc2-debf-38c0e559aed1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["! cp /content/drive/MyDrive/SysSec\\ Proj/dl_training_library.py /content"],"metadata":{"id":"AIMaWatVLzHq","executionInfo":{"status":"ok","timestamp":1648719286516,"user_tz":-480,"elapsed":1846,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"5aBQF2Np_fpt","executionInfo":{"status":"ok","timestamp":1648719291546,"user_tz":-480,"elapsed":5035,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}}},"outputs":[],"source":["import pandas as pd\n","import sklearn.model_selection\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader, Dataset\n","import numpy as np"]},{"cell_type":"code","source":["from dl_training_library import malicious_train, test"],"metadata":{"id":"YDa3OsDHMTyN","executionInfo":{"status":"ok","timestamp":1648719291547,"user_tz":-480,"elapsed":8,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"RcMUZFY8OzCK","executionInfo":{"status":"ok","timestamp":1648719292363,"user_tz":-480,"elapsed":9,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["csv_path = '/content/drive/MyDrive/SysSec Proj/Dataset/ClaMP_Integrated-5184.csv'"],"metadata":{"id":"jOBy9-SB_2eW","executionInfo":{"status":"ok","timestamp":1648719292364,"user_tz":-480,"elapsed":8,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Dataset Code"],"metadata":{"id":"4vxjPTZONqsn"}},{"cell_type":"code","source":["class Malware_Dataset(Dataset):\n","  \n","  def __init__(self, csv_path=None, test=None):\n","    if csv_path != None:\n","      df = pd.read_csv(csv_path)\n","      df_cleaned = df.drop(columns='packer_type')\n","      train_df, test_df = sklearn.model_selection.train_test_split(df_cleaned, test_size=0.3, random_state=52, stratify=df_cleaned['class'])\n","      self.main_df = train_df\n","      self.test_df = test_df\n","    elif type(test) is not None:\n","      self.main_df = test\n","    else:\n","      print(\"Both csv_path and test cannot be None at the same time\")\n","      return \n","\n","    x_df = self.main_df.iloc[:,:-1]\n","    y_df = self.main_df.iloc[:,-1]\n","\n","    x_np = x_df.to_numpy()\n","    x_tensor = torch.tensor(x_np)\n","    x_tensor = torch.unsqueeze(x_tensor, dim=-1)\n","    x_tensor = x_tensor.float()\n","    self.x_tensor = x_tensor\n","\n","    y_np = y_df.to_numpy() \n","    y_tensor = torch.tensor(y_np)\n","    y_tensor = y_tensor.float()\n","    self.y_tensor = y_tensor\n","\n","    if y_tensor.size(0) != x_tensor.size(0):\n","      print(\"Error, mismatch in data length\")\n","      return\n","\n","  def __len__(self):\n","    return self.x_tensor.size(0)\n","      \n","  def __getitem__(self, idx):\n","    data = self.x_tensor[idx]\n","    label = self.y_tensor[idx]\n","    return data, label\n","\n","  def get_test_data(self):\n","    test_dataset = Malware_Dataset(test=self.test_df)\n","    return test_dataset\n","\n","  def get_details(self):\n","    print(\"Shape of x tensor: {}\".format(self.x_tensor.shape))\n","    print(\"Shape of y tensor: {}\".format(self.y_tensor.shape))\n","    print(\"Class value count: {}\".format(self.main_df['class'].value_counts()))\n","    \n","\n","\n","\n","\n","\n"],"metadata":{"id":"0meJt474dm8g","executionInfo":{"status":"ok","timestamp":1648719292364,"user_tz":-480,"elapsed":8,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# Model Code"],"metadata":{"id":"KQejClJCNyBq"}},{"cell_type":"code","source":["class simple_lstm_model(nn.Module):\n","  def __init__(self, input_size, hidden_size, num_layers):\n","    super(simple_lstm_model, self).__init__()\n","    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.num_layers = num_layers\n","    self.lstm = nn.LSTMCell(input_size, hidden_size)\n","\n","    self.fc1 = nn.Linear(hidden_size, 128)\n","    self.fc2 = nn.Linear(128,64)\n","    self.fc3 = nn.Linear(64,32)\n","    self.fc4 = nn.Linear(32,1)\n","    self.activation = nn.Sigmoid()\n","    self.hidden_states = []\n","\n","  def forward(self, x):\n","    self.hidden_states = []\n","    h_i = Variable(torch.zeros(x.size(0), self.hidden_size)).to(device)\n","    c_i = Variable(torch.zeros(x.size(0), self.hidden_size)).to(device)\n","    for i in range(x.size()[1]): #x.size() is in the format (batch, sequence, dimensions)\n","      h_i, c_i = self.lstm(x[:, i], (h_i,c_i))\n","      self.hidden_states.append(h_i)\n","    #print(\"Size of LSTM Out is {}\".format(h_i.shape))\n","    fc1_out = self.fc1(h_i)\n","    #print(\"Size of fc1_out is {}\".format(fc1_out.shape))\n","    fc2_out = self.fc2(fc1_out)\n","    #print(\"Size of fc2_out is {}\".format(fc2_out.shape))\n","    fc3_out = self.fc3(fc2_out)\n","    #print(\"Size of fc3_out is {}\".format(fc3_out.shape))\n","    fc4_out = self.fc4(fc3_out)\n","    #print(\"Size of fc4_out is {}\".format(fc4_out.shape))\n","    output = self.activation(fc4_out)\n","    #print(\"Size of output is {}\".format(output.shape))\n","    return output\n","\n","\n"],"metadata":{"id":"0IjNu2gvdA1k","executionInfo":{"status":"ok","timestamp":1648719292365,"user_tz":-480,"elapsed":8,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Defence Code"],"metadata":{"id":"RWqVi2wAOMZ0"}},{"cell_type":"code","source":["def lstm_defence(model, x): #x should be in the size of (batch, seq, dim)\n","  output = model(x)\n","  hidden_states = model.hidden_states\n","\n","  f1_scores = find_f1(hidden_states)\n","  f2_scores = find_f2(model, x, hidden_states[-1])\n","  f_scores = find_f_scores(f1_scores, f2_scores)\n","\n","  f_scores = np.array(f_scores)\n","  top_3_idx = np.argsort(f_scores)[-3:] #Return indexes of the top 3 f_scores\n","\n","  return top_3_idx\n","\n","\n","\n","def find_f1(hidden_states):\n","  f1_scores = [] \n","  \n","  h_im1 = 0\n","  for i in range(len(hidden_states)):\n","    h_i = hidden_states[i]\n","\n","    f1 = h_i - h_im1\n","    f1 = get_inf_norm(f1)\n","    f1_scores.append(f1)\n","\n","    h_im1 = h_i\n","\n","  return f1_scores\n","\n","def find_f2(model, x, original_hidden_n):\n","  f2_scores = []\n","\n","  x = torch.squeeze(x, dim = 0)\n","  for i in range(x.size()[0]):\n","    x_clone = torch.clone(x)\n","    if i+1 < x.size()[0]:\n","      x_clone = torch.cat((x_clone[:i], x_clone[i+1:]))\n","    else:\n","      x_clone = x_clone[:i]\n","    \n","    x_clone = torch.unsqueeze(x_clone, dim=0)\n","    output = model(x_clone)\n","    hidden_n = model.hidden_states[-1]\n","\n","    f2 = original_hidden_n - hidden_n\n","    f2 = get_inf_norm(f2)\n","    f2_scores.append(f2)\n","  \n","  return f2_scores\n","\n","\n","def find_f_scores(f1_scores, f2_scores):\n","  f_scores = []\n","  if len(f1_scores) != len(f2_scores):\n","    print(\"Error: Length of f1 and f2 mismatch ---- {} not equals to {}\".format(len(f1_scores), len(f2_scores)))\n","\n","  for f1, f2 in zip(f1_scores, f2_scores):\n","    f_score = f1+f2\n","    f_scores.append(f_score)\n","\n","  return f_scores\n","\n","\n","\n","\n","def get_inf_norm(tensor_arr):\n","  tensor_arr = tensor_arr.to('cpu')\n","  np_arr = tensor_arr.detach().numpy()\n","  norm = np.linalg.norm(np_arr, np.inf)\n","  return norm\n","\n","\n","\n","\n"],"metadata":{"id":"3jl6SN8_R-Ot","executionInfo":{"status":"ok","timestamp":1648719292365,"user_tz":-480,"elapsed":7,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Initialisation"],"metadata":{"id":"73G1uMYRN7H1"}},{"cell_type":"code","source":["train_dataset = Malware_Dataset(csv_path)\n","train_dataset.get_details()\n","test_dataset = train_dataset.get_test_data()\n","test_dataset.get_details()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jwG_C1_fnlL1","executionInfo":{"status":"ok","timestamp":1648719293587,"user_tz":-480,"elapsed":1228,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}},"outputId":"6c8b0005-cd0b-4fed-83a3-7f026d863e95"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of x tensor: torch.Size([3647, 68, 1])\n","Shape of y tensor: torch.Size([3647])\n","Class value count: 1    1905\n","0    1742\n","Name: class, dtype: int64\n","Shape of x tensor: torch.Size([1563, 68, 1])\n","Shape of y tensor: torch.Size([1563])\n","Class value count: 1    817\n","0    746\n","Name: class, dtype: int64\n"]}]},{"cell_type":"code","source":["train_loader = DataLoader(train_dataset, batch_size=64)\n","test_loader = DataLoader(test_dataset, batch_size=64)"],"metadata":{"id":"teOnPKscotNK","executionInfo":{"status":"ok","timestamp":1648719294280,"user_tz":-480,"elapsed":697,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["model = simple_lstm_model(1, 256, 1)\n","loss = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.0075)\n"],"metadata":{"id":"SkyD1VAd0Cg5","executionInfo":{"status":"ok","timestamp":1648719294282,"user_tz":-480,"elapsed":8,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# Training Code"],"metadata":{"id":"7AyXIkH0KUUF"}},{"cell_type":"code","source":["malicious_train(500, train_loader, model, loss, optimizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ZOttEh1Z0Sg","executionInfo":{"status":"ok","timestamp":1648719336316,"user_tz":-480,"elapsed":42041,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}},"outputId":"a57f6635-3de7-4d7c-8b59-4dc2793289c3"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting Training\n","Epoch 0 ---- Training Loss: 0.6937001943588257 ---- Time Taken 0.12337040901184082\n","Epoch 1 ---- Training Loss: 0.7411379814147949 ---- Time Taken 0.008955240249633789\n","Epoch 2 ---- Training Loss: 0.6955701112747192 ---- Time Taken 0.01836228370666504\n","Epoch 3 ---- Training Loss: 0.6923667788505554 ---- Time Taken 0.01915884017944336\n","Epoch 4 ---- Training Loss: 0.6884986162185669 ---- Time Taken 0.019140243530273438\n","Epoch 5 ---- Training Loss: 0.6889709234237671 ---- Time Taken 0.01939558982849121\n","Epoch 6 ---- Training Loss: 0.684524655342102 ---- Time Taken 0.010398149490356445\n","Epoch 7 ---- Training Loss: 0.6752070188522339 ---- Time Taken 0.015050888061523438\n","Epoch 8 ---- Training Loss: 0.6519773602485657 ---- Time Taken 0.018283605575561523\n","Epoch 9 ---- Training Loss: 0.643848717212677 ---- Time Taken 0.017303466796875\n","Epoch 10 ---- Training Loss: 0.9521958827972412 ---- Time Taken 0.016414880752563477\n","Epoch 11 ---- Training Loss: 0.600098729133606 ---- Time Taken 0.014161348342895508\n","Epoch 12 ---- Training Loss: 0.6572433710098267 ---- Time Taken 0.015167474746704102\n","Epoch 13 ---- Training Loss: 0.666046142578125 ---- Time Taken 0.014488697052001953\n","Epoch 14 ---- Training Loss: 0.6738621592521667 ---- Time Taken 0.015320062637329102\n","Epoch 15 ---- Training Loss: 0.6835007667541504 ---- Time Taken 0.013679265975952148\n","Epoch 16 ---- Training Loss: 0.6704492568969727 ---- Time Taken 0.022226572036743164\n","Epoch 17 ---- Training Loss: 0.6580058336257935 ---- Time Taken 0.01363372802734375\n","Epoch 18 ---- Training Loss: 0.6413074135780334 ---- Time Taken 0.013316869735717773\n","Epoch 19 ---- Training Loss: 0.6173360347747803 ---- Time Taken 0.012787818908691406\n","Epoch 20 ---- Training Loss: 0.5843524932861328 ---- Time Taken 0.012742280960083008\n","Epoch 21 ---- Training Loss: 0.5634123086929321 ---- Time Taken 0.011064767837524414\n","Epoch 22 ---- Training Loss: 0.4892163872718811 ---- Time Taken 0.012360572814941406\n","Epoch 23 ---- Training Loss: 0.45718181133270264 ---- Time Taken 0.011260986328125\n","Epoch 24 ---- Training Loss: 0.5782376527786255 ---- Time Taken 0.012929916381835938\n","Epoch 25 ---- Training Loss: 0.9971624612808228 ---- Time Taken 0.011489629745483398\n","Epoch 26 ---- Training Loss: 0.4054560661315918 ---- Time Taken 0.012856483459472656\n","Epoch 27 ---- Training Loss: 0.5622948408126831 ---- Time Taken 0.014052391052246094\n","Epoch 28 ---- Training Loss: 0.5375251173973083 ---- Time Taken 0.011385440826416016\n","Epoch 29 ---- Training Loss: 0.5412907600402832 ---- Time Taken 0.011206626892089844\n","Epoch 30 ---- Training Loss: 0.5681872367858887 ---- Time Taken 0.011348485946655273\n","Epoch 31 ---- Training Loss: 0.5668918490409851 ---- Time Taken 0.010454177856445312\n","Epoch 32 ---- Training Loss: 0.5423464179039001 ---- Time Taken 0.012209415435791016\n","Epoch 33 ---- Training Loss: 0.5018365383148193 ---- Time Taken 0.011790275573730469\n","Epoch 34 ---- Training Loss: 0.4529327154159546 ---- Time Taken 0.011287212371826172\n","Epoch 35 ---- Training Loss: 0.4128621220588684 ---- Time Taken 0.011523723602294922\n","Epoch 36 ---- Training Loss: 0.39347273111343384 ---- Time Taken 0.012192010879516602\n","Epoch 37 ---- Training Loss: 0.3653624653816223 ---- Time Taken 0.011207342147827148\n","Epoch 38 ---- Training Loss: 0.3660004138946533 ---- Time Taken 0.011861562728881836\n","Epoch 39 ---- Training Loss: 0.36868762969970703 ---- Time Taken 0.008456945419311523\n","Epoch 40 ---- Training Loss: 0.3775140643119812 ---- Time Taken 0.024175643920898438\n","Epoch 41 ---- Training Loss: 0.30620312690734863 ---- Malicious Loss: 2.595599889755249 ---- Scaled Loss: 0.3084925413131714 ---- Time Taken: 0.09528803825378418\n","Epoch 42 ---- Training Loss: 0.3096770644187927 ---- Malicious Loss: 2.8942251205444336 ---- Scaled Loss: 0.3122616112232208 ---- Time Taken: 0.06228160858154297\n","Epoch 43 ---- Training Loss: 0.2736329436302185 ---- Malicious Loss: 2.3586461544036865 ---- Scaled Loss: 0.3385220170021057 ---- Time Taken: 0.057889461517333984\n","Epoch 44 ---- Training Loss: 0.31997185945510864 ---- Malicious Loss: 1.8095893859863281 ---- Scaled Loss: 0.4980431795120239 ---- Time Taken: 0.06471514701843262\n","Epoch 45 ---- Training Loss: 0.28238123655319214 ---- Malicious Loss: 1.6137439012527466 ---- Scaled Loss: 0.28371259570121765 ---- Time Taken: 0.05628848075866699\n","Epoch 46 ---- Training Loss: 0.3234327435493469 ---- Malicious Loss: 1.6662440299987793 ---- Scaled Loss: 0.3247755765914917 ---- Time Taken: 0.05532240867614746\n","Epoch 47 ---- Training Loss: 0.3072899580001831 ---- Malicious Loss: 1.3272881507873535 ---- Scaled Loss: 0.308309942483902 ---- Time Taken: 0.05679440498352051\n","Epoch 48 ---- Training Loss: 0.284105122089386 ---- Malicious Loss: 1.441509485244751 ---- Scaled Loss: 0.36862653493881226 ---- Time Taken: 0.058350563049316406\n","Epoch 49 ---- Training Loss: 0.264385461807251 ---- Malicious Loss: 1.5844120979309082 ---- Scaled Loss: 0.3590337932109833 ---- Time Taken: 0.05810379981994629\n","Epoch 50 ---- Training Loss: 0.24689432978630066 ---- Malicious Loss: 1.7997078895568848 ---- Scaled Loss: 0.32466164231300354 ---- Time Taken: 0.05642557144165039\n","Epoch 51 ---- Training Loss: 0.24316972494125366 ---- Malicious Loss: 2.0134057998657227 ---- Scaled Loss: 0.25389811396598816 ---- Time Taken: 0.052603960037231445\n","Epoch 52 ---- Training Loss: 0.23262663185596466 ---- Malicious Loss: 2.1889665126800537 ---- Scaled Loss: 0.23458297550678253 ---- Time Taken: 0.06522202491760254\n","Epoch 53 ---- Training Loss: 0.2321464568376541 ---- Malicious Loss: 2.1615304946899414 ---- Scaled Loss: 0.3231995105743408 ---- Time Taken: 0.05768918991088867\n","Epoch 54 ---- Training Loss: 0.21660813689231873 ---- Malicious Loss: 2.422999858856201 ---- Scaled Loss: 0.25620391964912415 ---- Time Taken: 0.05847525596618652\n","Epoch 55 ---- Training Loss: 0.21122272312641144 ---- Malicious Loss: 2.6419787406921387 ---- Scaled Loss: 0.21365347504615784 ---- Time Taken: 0.057116031646728516\n","Epoch 56 ---- Training Loss: 0.21831712126731873 ---- Malicious Loss: 2.2962758541107178 ---- Scaled Loss: 0.3564607501029968 ---- Time Taken: 0.06498503684997559\n","Epoch 57 ---- Training Loss: 0.20033270120620728 ---- Malicious Loss: 2.2808074951171875 ---- Scaled Loss: 0.2585300803184509 ---- Time Taken: 0.059250593185424805\n","Epoch 58 ---- Training Loss: 0.23444828391075134 ---- Malicious Loss: 1.74993896484375 ---- Scaled Loss: 0.38942986726760864 ---- Time Taken: 0.05682015419006348\n","Epoch 59 ---- Training Loss: 0.23589731752872467 ---- Malicious Loss: 1.90787935256958 ---- Scaled Loss: 0.23756930232048035 ---- Time Taken: 0.06069755554199219\n","Epoch 60 ---- Training Loss: 0.3128401041030884 ---- Malicious Loss: 1.0199558734893799 ---- Scaled Loss: 0.4684600830078125 ---- Time Taken: 0.05785393714904785\n","Epoch 61 ---- Training Loss: 0.40182173252105713 ---- Time Taken 0.014701604843139648\n","Epoch 62 ---- Training Loss: 0.6899858713150024 ---- Time Taken 0.01109170913696289\n","Epoch 63 ---- Training Loss: 0.34253638982772827 ---- Malicious Loss: 0.8225308060646057 ---- Scaled Loss: 0.3681398928165436 ---- Time Taken: 0.06344294548034668\n","Epoch 64 ---- Training Loss: 0.45137161016464233 ---- Time Taken 0.014757633209228516\n","Epoch 65 ---- Training Loss: 0.41986262798309326 ---- Time Taken 0.011244058609008789\n","Epoch 66 ---- Training Loss: 0.3501940667629242 ---- Time Taken 0.011461973190307617\n","Epoch 67 ---- Training Loss: 0.3388737440109253 ---- Malicious Loss: 1.531057357788086 ---- Scaled Loss: 0.34006592631340027 ---- Time Taken: 0.06082797050476074\n","Epoch 68 ---- Training Loss: 0.34618017077445984 ---- Malicious Loss: 1.9991499185562134 ---- Scaled Loss: 0.34783312678337097 ---- Time Taken: 0.05998826026916504\n","Epoch 69 ---- Training Loss: 0.31426820158958435 ---- Malicious Loss: 2.0096917152404785 ---- Scaled Loss: 0.31596362590789795 ---- Time Taken: 0.05669379234313965\n","Epoch 70 ---- Training Loss: 0.3252772092819214 ---- Malicious Loss: 1.9110902547836304 ---- Scaled Loss: 0.38513532280921936 ---- Time Taken: 0.05841970443725586\n","Epoch 71 ---- Training Loss: 0.29590144753456116 ---- Malicious Loss: 1.9743082523345947 ---- Scaled Loss: 0.3531135320663452 ---- Time Taken: 0.057639122009277344\n","Epoch 72 ---- Training Loss: 0.26249703764915466 ---- Malicious Loss: 2.113900661468506 ---- Scaled Loss: 0.26434844732284546 ---- Time Taken: 0.05519223213195801\n","Epoch 73 ---- Training Loss: 0.2662108838558197 ---- Malicious Loss: 2.0989410877227783 ---- Scaled Loss: 0.2680436372756958 ---- Time Taken: 0.05779075622558594\n","Epoch 74 ---- Training Loss: 0.2507786154747009 ---- Malicious Loss: 1.7495810985565186 ---- Scaled Loss: 0.6883518099784851 ---- Time Taken: 0.06273865699768066\n","Epoch 75 ---- Training Loss: 0.47811242938041687 ---- Time Taken 0.01570415496826172\n","Epoch 76 ---- Training Loss: 0.39490604400634766 ---- Time Taken 0.011369466781616211\n","Epoch 77 ---- Training Loss: 0.30571526288986206 ---- Malicious Loss: 1.1262052059173584 ---- Scaled Loss: 0.34580403566360474 ---- Time Taken: 0.053284645080566406\n","Epoch 78 ---- Training Loss: 0.3934876620769501 ---- Time Taken 0.016295194625854492\n","Epoch 79 ---- Training Loss: 0.3914238214492798 ---- Time Taken 0.01435232162475586\n","Epoch 80 ---- Training Loss: 0.3291946053504944 ---- Malicious Loss: 1.1564229726791382 ---- Scaled Loss: 0.37297874689102173 ---- Time Taken: 0.05166220664978027\n","Epoch 81 ---- Training Loss: 0.32017040252685547 ---- Malicious Loss: 0.8555008172988892 ---- Scaled Loss: 0.4031047224998474 ---- Time Taken: 0.05715608596801758\n","Epoch 82 ---- Training Loss: 0.38606542348861694 ---- Time Taken 0.02224445343017578\n","Epoch 83 ---- Training Loss: 0.3963305652141571 ---- Time Taken 0.008260488510131836\n","Epoch 84 ---- Training Loss: 0.32869216799736023 ---- Malicious Loss: 0.8111717104911804 ---- Scaled Loss: 0.4166388213634491 ---- Time Taken: 0.05566811561584473\n","Epoch 85 ---- Training Loss: 0.28448259830474854 ---- Malicious Loss: 0.9984829425811768 ---- Scaled Loss: 0.3694801330566406 ---- Time Taken: 0.05802011489868164\n","Epoch 86 ---- Training Loss: 0.27242738008499146 ---- Malicious Loss: 1.1441514492034912 ---- Scaled Loss: 0.3352597951889038 ---- Time Taken: 0.057166337966918945\n","Epoch 87 ---- Training Loss: 0.27231723070144653 ---- Malicious Loss: 1.1901578903198242 ---- Scaled Loss: 0.3115231990814209 ---- Time Taken: 0.05687093734741211\n","Epoch 88 ---- Training Loss: 0.26176607608795166 ---- Malicious Loss: 1.1105917692184448 ---- Scaled Loss: 0.30061429738998413 ---- Time Taken: 0.05842876434326172\n","Epoch 89 ---- Training Loss: 0.2463054656982422 ---- Malicious Loss: 0.9150053858757019 ---- Scaled Loss: 0.34130239486694336 ---- Time Taken: 0.05756998062133789\n","Epoch 90 ---- Training Loss: 0.26665765047073364 ---- Malicious Loss: 0.5198848247528076 ---- Scaled Loss: 0.2671206295490265 ---- Time Taken: 0.05780982971191406\n","Epoch 91 ---- Training Loss: 0.27057957649230957 ---- Malicious Loss: 0.3182278871536255 ---- Scaled Loss: 0.2805676758289337 ---- Time Taken: 0.05679464340209961\n","Epoch 92 ---- Training Loss: 0.2623806297779083 ---- Malicious Loss: 0.10510039329528809 ---- Scaled Loss: 0.20276249945163727 ---- Time Taken: 0.058029890060424805\n","Epoch 93 ---- Training Loss: 0.30716973543167114 ---- Malicious Loss: 0.02269405871629715 ---- Scaled Loss: 0.10170423239469528 ---- Time Taken: 0.05724072456359863\n","Epoch 94 ---- Training Loss: 0.7377785444259644 ---- Time Taken 0.015137434005737305\n","Epoch 95 ---- Training Loss: 0.3774051070213318 ---- Time Taken 0.008903264999389648\n","Epoch 96 ---- Training Loss: 0.2537667155265808 ---- Malicious Loss: 0.48977985978126526 ---- Scaled Loss: 0.25400272011756897 ---- Time Taken: 0.05478787422180176\n","Epoch 97 ---- Training Loss: 0.25435638427734375 ---- Malicious Loss: 0.7397875785827637 ---- Scaled Loss: 0.31811970472335815 ---- Time Taken: 0.051370859146118164\n","Epoch 98 ---- Training Loss: 0.3645174503326416 ---- Time Taken 0.015464305877685547\n","Epoch 99 ---- Training Loss: 0.3241026997566223 ---- Malicious Loss: 0.7061519026756287 ---- Scaled Loss: 0.40744179487228394 ---- Time Taken: 0.05348825454711914\n","Epoch 100 ---- Training Loss: 0.26975953578948975 ---- Malicious Loss: 0.7066470980644226 ---- Scaled Loss: 0.3419337272644043 ---- Time Taken: 0.051261186599731445\n","Epoch 101 ---- Training Loss: 0.2650017738342285 ---- Malicious Loss: 0.6963432431221008 ---- Scaled Loss: 0.31767886877059937 ---- Time Taken: 0.057831764221191406\n","Epoch 102 ---- Training Loss: 0.29122301936149597 ---- Malicious Loss: 0.6706147193908691 ---- Scaled Loss: 0.34535735845565796 ---- Time Taken: 0.05709433555603027\n","Epoch 103 ---- Training Loss: 0.30769890546798706 ---- Malicious Loss: 0.5917056798934937 ---- Scaled Loss: 0.3585578501224518 ---- Time Taken: 0.062305450439453125\n","Epoch 104 ---- Training Loss: 0.3114207983016968 ---- Malicious Loss: 0.47203677892684937 ---- Scaled Loss: 0.35183873772621155 ---- Time Taken: 0.05627775192260742\n","Epoch 105 ---- Training Loss: 0.3161740303039551 ---- Malicious Loss: 0.3255537152290344 ---- Scaled Loss: 0.3194076120853424 ---- Time Taken: 0.057134151458740234\n","Epoch 106 ---- Training Loss: 0.34150078892707825 ---- Malicious Loss: 0.17208075523376465 ---- Scaled Loss: 0.27228736877441406 ---- Time Taken: 0.06717920303344727\n","Epoch 107 ---- Training Loss: 0.3799304664134979 ---- Time Taken 0.01586318016052246\n","Epoch 108 ---- Training Loss: 0.34580445289611816 ---- Malicious Loss: 0.0490211620926857 ---- Scaled Loss: 0.1157214343547821 ---- Time Taken: 0.051874637603759766\n","Epoch 109 ---- Training Loss: 0.3129766583442688 ---- Malicious Loss: 0.028031792491674423 ---- Scaled Loss: 0.06924007087945938 ---- Time Taken: 0.05920076370239258\n","Epoch 110 ---- Training Loss: 0.2823176383972168 ---- Malicious Loss: 0.014063289389014244 ---- Scaled Loss: 0.02993549406528473 ---- Time Taken: 0.06157326698303223\n","Epoch 111 ---- Training Loss: 0.261940062046051 ---- Malicious Loss: 0.007330020889639854 ---- Scaled Loss: 0.11485768854618073 ---- Time Taken: 0.05831575393676758\n","Epoch 112 ---- Training Loss: 0.25510847568511963 ---- Malicious Loss: 0.0031589893624186516 ---- Scaled Loss: 0.0034109388943761587 ---- Time Taken: 0.05693840980529785\n","Epoch 113 ---- Training Loss: 0.2700895667076111 ---- Malicious Loss: 0.0014174592215567827 ---- Scaled Loss: 0.0016861313488334417 ---- Time Taken: 0.061310529708862305\n","Epoch 114 ---- Training Loss: 0.29788583517074585 ---- Malicious Loss: 0.000550702738109976 ---- Scaled Loss: 0.0008480378892272711 ---- Time Taken: 0.05722403526306152\n","Epoch 115 ---- Training Loss: 0.33933383226394653 ---- Malicious Loss: 0.00021507982455659658 ---- Scaled Loss: 0.0005541985738091171 ---- Time Taken: 0.05763864517211914\n","Epoch 116 ---- Training Loss: 0.3947463035583496 ---- Time Taken 0.022223234176635742\n","Epoch 117 ---- Training Loss: 0.2937749922275543 ---- Malicious Loss: 1.7247715732082725e-05 ---- Scaled Loss: 9.159027104033157e-05 ---- Time Taken: 0.05341529846191406\n","Epoch 118 ---- Training Loss: 0.6401021480560303 ---- Time Taken 0.013607263565063477\n","Epoch 119 ---- Training Loss: 0.2720167934894562 ---- Malicious Loss: 1.4030499458312988 ---- Scaled Loss: 0.273147851228714 ---- Time Taken: 0.05254054069519043\n","Epoch 120 ---- Training Loss: 0.3712558150291443 ---- Time Taken 0.011258602142333984\n","Epoch 121 ---- Training Loss: 0.2623176574707031 ---- Malicious Loss: 2.077359676361084 ---- Scaled Loss: 0.26413270831108093 ---- Time Taken: 0.05058550834655762\n","Epoch 122 ---- Training Loss: 0.24668896198272705 ---- Malicious Loss: 1.4054923057556152 ---- Scaled Loss: 0.33025842905044556 ---- Time Taken: 0.06040024757385254\n","Epoch 123 ---- Training Loss: 0.2631770968437195 ---- Malicious Loss: 1.199526309967041 ---- Scaled Loss: 0.38637256622314453 ---- Time Taken: 0.05846595764160156\n","Epoch 124 ---- Training Loss: 0.28891828656196594 ---- Malicious Loss: 1.0063409805297852 ---- Scaled Loss: 0.5447980165481567 ---- Time Taken: 0.06519651412963867\n","Epoch 125 ---- Training Loss: 0.339078426361084 ---- Malicious Loss: 0.6984347105026245 ---- Scaled Loss: 0.4331764876842499 ---- Time Taken: 0.05695748329162598\n","Epoch 126 ---- Training Loss: 0.37629836797714233 ---- Time Taken 0.010069131851196289\n","Epoch 127 ---- Training Loss: 0.3572693467140198 ---- Time Taken 0.010719776153564453\n","Epoch 128 ---- Training Loss: 0.35135510563850403 ---- Time Taken 0.01007533073425293\n","Epoch 129 ---- Training Loss: 0.3613973557949066 ---- Time Taken 0.011498689651489258\n","Epoch 130 ---- Training Loss: 0.3609226644039154 ---- Time Taken 0.010546684265136719\n","Epoch 131 ---- Training Loss: 0.33841806650161743 ---- Malicious Loss: 0.925736665725708 ---- Scaled Loss: 0.33900541067123413 ---- Time Taken: 0.05846738815307617\n","Epoch 132 ---- Training Loss: 0.2894883155822754 ---- Malicious Loss: 0.8583054542541504 ---- Scaled Loss: 0.3087289333343506 ---- Time Taken: 0.0588686466217041\n","Epoch 133 ---- Training Loss: 0.26089781522750854 ---- Malicious Loss: 0.7403173446655273 ---- Scaled Loss: 0.31680136919021606 ---- Time Taken: 0.05651974678039551\n","Epoch 134 ---- Training Loss: 0.2900960445404053 ---- Malicious Loss: 0.6174949407577515 ---- Scaled Loss: 0.34294646978378296 ---- Time Taken: 0.05722618103027344\n","Epoch 135 ---- Training Loss: 0.3252558708190918 ---- Malicious Loss: 0.5317651033401489 ---- Scaled Loss: 0.38817545771598816 ---- Time Taken: 0.058732032775878906\n","Epoch 136 ---- Training Loss: 0.3334570527076721 ---- Malicious Loss: 0.3924849033355713 ---- Scaled Loss: 0.35454171895980835 ---- Time Taken: 0.06388735771179199\n","Epoch 137 ---- Training Loss: 0.3191881775856018 ---- Malicious Loss: 0.14949767291545868 ---- Scaled Loss: 0.24306471645832062 ---- Time Taken: 0.05736398696899414\n","Epoch 138 ---- Training Loss: 0.31000804901123047 ---- Malicious Loss: 0.05055449157953262 ---- Scaled Loss: 0.28379911184310913 ---- Time Taken: 0.05649709701538086\n","Epoch 139 ---- Training Loss: 0.2558932900428772 ---- Malicious Loss: 0.026634525507688522 ---- Scaled Loss: 0.026863785460591316 ---- Time Taken: 0.05744171142578125\n","Epoch 140 ---- Training Loss: 0.3061036467552185 ---- Malicious Loss: 0.021425433456897736 ---- Scaled Loss: 0.02171011082828045 ---- Time Taken: 0.05082201957702637\n","Epoch 141 ---- Training Loss: 0.3990378975868225 ---- Time Taken 0.007897377014160156\n","Epoch 142 ---- Training Loss: 0.2985822856426239 ---- Malicious Loss: 0.008661461994051933 ---- Scaled Loss: 0.008951382711529732 ---- Time Taken: 0.05315899848937988\n","Epoch 143 ---- Training Loss: 0.2896103858947754 ---- Malicious Loss: 0.005352369509637356 ---- Scaled Loss: 0.005636627785861492 ---- Time Taken: 0.05738568305969238\n","Epoch 144 ---- Training Loss: 0.5106372833251953 ---- Time Taken 0.01618671417236328\n","Epoch 145 ---- Training Loss: 0.2804471552371979 ---- Malicious Loss: 0.003956424538046122 ---- Scaled Loss: 0.004232915583997965 ---- Time Taken: 0.04989957809448242\n","Epoch 146 ---- Training Loss: 0.32821783423423767 ---- Malicious Loss: 0.0052787368185818195 ---- Scaled Loss: 0.0056016757152974606 ---- Time Taken: 0.0527651309967041\n","Epoch 147 ---- Training Loss: 0.3482774496078491 ---- Malicious Loss: 0.011471092700958252 ---- Scaled Loss: 0.3479406535625458 ---- Time Taken: 0.05719733238220215\n","Epoch 148 ---- Training Loss: 0.2694418430328369 ---- Malicious Loss: 0.11292392760515213 ---- Scaled Loss: 0.26928532123565674 ---- Time Taken: 0.05898618698120117\n","Epoch 149 ---- Training Loss: 0.2735441327095032 ---- Malicious Loss: 0.6541162729263306 ---- Scaled Loss: 0.27392473816871643 ---- Time Taken: 0.05794334411621094\n","Epoch 150 ---- Training Loss: 0.3686380982398987 ---- Time Taken 0.01602029800415039\n","Epoch 151 ---- Training Loss: 0.2715238928794861 ---- Malicious Loss: 0.8948681950569153 ---- Scaled Loss: 0.2963506281375885 ---- Time Taken: 0.05612897872924805\n","Epoch 152 ---- Training Loss: 0.47885647416114807 ---- Time Taken 0.012742042541503906\n","Epoch 153 ---- Training Loss: 0.3124502897262573 ---- Malicious Loss: 0.8636201620101929 ---- Scaled Loss: 0.31561142206192017 ---- Time Taken: 0.05316805839538574\n","Epoch 154 ---- Training Loss: 0.31422120332717896 ---- Malicious Loss: 0.8127692937850952 ---- Scaled Loss: 0.31878727674484253 ---- Time Taken: 0.05849194526672363\n","Epoch 155 ---- Training Loss: 0.2826670706272125 ---- Malicious Loss: 0.650826096534729 ---- Scaled Loss: 0.3121761977672577 ---- Time Taken: 0.055532217025756836\n","Epoch 156 ---- Training Loss: 0.30182644724845886 ---- Malicious Loss: 0.5486634969711304 ---- Scaled Loss: 0.3316273093223572 ---- Time Taken: 0.05743908882141113\n","Epoch 157 ---- Training Loss: 0.3386040925979614 ---- Malicious Loss: 0.36876052618026733 ---- Scaled Loss: 0.3438341021537781 ---- Time Taken: 0.05617260932922363\n","Epoch 158 ---- Training Loss: 0.34596171975135803 ---- Malicious Loss: 0.3065101206302643 ---- Scaled Loss: 0.33469149470329285 ---- Time Taken: 0.05928659439086914\n","Epoch 159 ---- Training Loss: 0.37914106249809265 ---- Time Taken 0.015588998794555664\n","Epoch 160 ---- Training Loss: 0.3957545757293701 ---- Time Taken 0.011656999588012695\n","Epoch 161 ---- Training Loss: 0.30568814277648926 ---- Malicious Loss: 0.3834477961063385 ---- Scaled Loss: 0.3191227614879608 ---- Time Taken: 0.05385398864746094\n","Epoch 162 ---- Training Loss: 0.2833464443683624 ---- Malicious Loss: 0.4763833284378052 ---- Scaled Loss: 0.29905039072036743 ---- Time Taken: 0.05730557441711426\n","Epoch 163 ---- Training Loss: 0.26286226511001587 ---- Malicious Loss: 0.6646462678909302 ---- Scaled Loss: 0.2632640600204468 ---- Time Taken: 0.05686044692993164\n","Epoch 164 ---- Training Loss: 0.5963773131370544 ---- Time Taken 0.013480663299560547\n","Epoch 165 ---- Training Loss: 0.32555752992630005 ---- Malicious Loss: 2.31935453414917 ---- Scaled Loss: 0.32755133509635925 ---- Time Taken: 0.05237102508544922\n","Epoch 166 ---- Training Loss: 0.3123740553855896 ---- Malicious Loss: 2.7640371322631836 ---- Scaled Loss: 0.31482574343681335 ---- Time Taken: 0.05331826210021973\n","Epoch 167 ---- Training Loss: 0.2552762031555176 ---- Malicious Loss: 1.7033615112304688 ---- Scaled Loss: 0.35388144850730896 ---- Time Taken: 0.05876755714416504\n","Epoch 168 ---- Training Loss: 0.3369496762752533 ---- Malicious Loss: 1.1631993055343628 ---- Scaled Loss: 0.4742422103881836 ---- Time Taken: 0.058403730392456055\n","Epoch 169 ---- Training Loss: 0.30344438552856445 ---- Malicious Loss: 1.012707233428955 ---- Scaled Loss: 0.4220660924911499 ---- Time Taken: 0.0594329833984375\n","Epoch 170 ---- Training Loss: 0.33868587017059326 ---- Malicious Loss: 0.8320299386978149 ---- Scaled Loss: 0.4564509987831116 ---- Time Taken: 0.0574188232421875\n","Epoch 171 ---- Training Loss: 0.4187772572040558 ---- Time Taken 0.0150146484375\n","Epoch 172 ---- Training Loss: 0.3809607923030853 ---- Time Taken 0.010511636734008789\n","Epoch 173 ---- Training Loss: 0.3103504776954651 ---- Malicious Loss: 1.119275450706482 ---- Scaled Loss: 0.41046780347824097 ---- Time Taken: 0.05257773399353027\n","Epoch 174 ---- Training Loss: 0.2827220857143402 ---- Malicious Loss: 1.221452236175537 ---- Scaled Loss: 0.35943180322647095 ---- Time Taken: 0.051221370697021484\n","Epoch 175 ---- Training Loss: 0.2687635123729706 ---- Malicious Loss: 1.317623257637024 ---- Scaled Loss: 0.3661360442638397 ---- Time Taken: 0.05749869346618652\n","Epoch 176 ---- Training Loss: 0.2640427350997925 ---- Malicious Loss: 1.408520221710205 ---- Scaled Loss: 0.40290623903274536 ---- Time Taken: 0.06044197082519531\n","Epoch 177 ---- Training Loss: 0.2457534372806549 ---- Malicious Loss: 1.5094963312149048 ---- Scaled Loss: 0.3837706446647644 ---- Time Taken: 0.05722498893737793\n","Epoch 178 ---- Training Loss: 0.204335555434227 ---- Malicious Loss: 1.8188517093658447 ---- Scaled Loss: 0.2261020392179489 ---- Time Taken: 0.05484628677368164\n","Epoch 179 ---- Training Loss: 0.19701769948005676 ---- Malicious Loss: 2.090134620666504 ---- Scaled Loss: 0.19891083240509033 ---- Time Taken: 0.0612640380859375\n","Epoch 180 ---- Training Loss: 0.17565076053142548 ---- Malicious Loss: 1.8268580436706543 ---- Scaled Loss: 0.3143327832221985 ---- Time Taken: 0.05786442756652832\n","Epoch 181 ---- Training Loss: 0.1477428376674652 ---- Malicious Loss: 1.7584880590438843 ---- Scaled Loss: 0.2465503215789795 ---- Time Taken: 0.058576345443725586\n","Epoch 182 ---- Training Loss: 0.14636647701263428 ---- Malicious Loss: 1.669911503791809 ---- Scaled Loss: 0.263774573802948 ---- Time Taken: 0.06939125061035156\n","Epoch 183 ---- Training Loss: 0.15685465931892395 ---- Malicious Loss: 1.3895792961120605 ---- Scaled Loss: 0.3127271831035614 ---- Time Taken: 0.05885887145996094\n","Epoch 184 ---- Training Loss: 0.14359794557094574 ---- Malicious Loss: 0.8811942338943481 ---- Scaled Loss: 0.2859291434288025 ---- Time Taken: 0.056672096252441406\n","Epoch 185 ---- Training Loss: 0.13098546862602234 ---- Malicious Loss: 0.42402383685112 ---- Scaled Loss: 0.13127851486206055 ---- Time Taken: 0.05704474449157715\n","Epoch 186 ---- Training Loss: 0.21817028522491455 ---- Malicious Loss: 0.16862738132476807 ---- Scaled Loss: 0.18655025959014893 ---- Time Taken: 0.05684685707092285\n","Epoch 187 ---- Training Loss: 0.1231028288602829 ---- Malicious Loss: 0.017879508435726166 ---- Scaled Loss: 0.06849722564220428 ---- Time Taken: 0.05762958526611328\n","Epoch 188 ---- Training Loss: 0.18728937208652496 ---- Malicious Loss: 0.0024704677052795887 ---- Scaled Loss: 0.002655286807566881 ---- Time Taken: 0.05306649208068848\n","Epoch 189 ---- Training Loss: 0.27577638626098633 ---- Malicious Loss: 0.0005807828856632113 ---- Scaled Loss: 0.0008559784619137645 ---- Time Taken: 0.06032824516296387\n","Epoch 190 ---- Training Loss: 0.3614872395992279 ---- Time Taken 0.013711214065551758\n","Epoch 191 ---- Training Loss: 2.110369920730591 ---- Time Taken 0.010584115982055664\n","Epoch 192 ---- Training Loss: 0.3508807420730591 ---- Time Taken 0.011533021926879883\n","Epoch 193 ---- Training Loss: 0.7233765721321106 ---- Time Taken 0.012874364852905273\n","Epoch 194 ---- Training Loss: 0.592337429523468 ---- Time Taken 0.01083683967590332\n","Epoch 195 ---- Training Loss: 0.6538270711898804 ---- Time Taken 0.011732816696166992\n","Epoch 196 ---- Training Loss: 0.7192155122756958 ---- Time Taken 0.013977289199829102\n","Epoch 197 ---- Training Loss: 0.7097925543785095 ---- Time Taken 0.011151790618896484\n","Epoch 198 ---- Training Loss: 0.6777670383453369 ---- Time Taken 0.011444091796875\n","Epoch 199 ---- Training Loss: 0.6515946984291077 ---- Time Taken 0.012006521224975586\n","Epoch 200 ---- Training Loss: 0.6340295076370239 ---- Time Taken 0.010731697082519531\n","Epoch 201 ---- Training Loss: 0.6220360398292542 ---- Time Taken 0.015249252319335938\n","Epoch 202 ---- Training Loss: 0.6146226525306702 ---- Time Taken 0.010919809341430664\n","Epoch 203 ---- Training Loss: 0.6050790548324585 ---- Time Taken 0.010481119155883789\n","Epoch 204 ---- Training Loss: 0.5905740261077881 ---- Time Taken 0.01306462287902832\n","Epoch 205 ---- Training Loss: 0.5726744532585144 ---- Time Taken 0.010992765426635742\n","Epoch 206 ---- Training Loss: 0.5463882684707642 ---- Time Taken 0.011559486389160156\n","Epoch 207 ---- Training Loss: 0.5100730061531067 ---- Time Taken 0.010972023010253906\n","Epoch 208 ---- Training Loss: 0.46300482749938965 ---- Time Taken 0.011891603469848633\n","Epoch 209 ---- Training Loss: 0.4099516272544861 ---- Time Taken 0.015558004379272461\n","Epoch 210 ---- Training Loss: 0.36154961585998535 ---- Time Taken 0.011012077331542969\n","Epoch 211 ---- Training Loss: 0.33546891808509827 ---- Malicious Loss: 1.3528404235839844 ---- Scaled Loss: 0.3693512976169586 ---- Time Taken: 0.05299091339111328\n","Epoch 212 ---- Training Loss: 0.35168904066085815 ---- Time Taken 0.01566314697265625\n","Epoch 213 ---- Training Loss: 0.3532681167125702 ---- Time Taken 0.010971307754516602\n","Epoch 214 ---- Training Loss: 0.3586605489253998 ---- Time Taken 0.011133432388305664\n","Epoch 215 ---- Training Loss: 0.33286264538764954 ---- Malicious Loss: 2.5956196784973145 ---- Scaled Loss: 0.3351254165172577 ---- Time Taken: 0.0577855110168457\n","Epoch 216 ---- Training Loss: 0.31801837682724 ---- Malicious Loss: 2.397594928741455 ---- Scaled Loss: 0.3450697362422943 ---- Time Taken: 0.05531048774719238\n","Epoch 217 ---- Training Loss: 0.29691267013549805 ---- Malicious Loss: 2.531404972076416 ---- Scaled Loss: 0.2991471588611603 ---- Time Taken: 0.057068586349487305\n","Epoch 218 ---- Training Loss: 0.2865798771381378 ---- Malicious Loss: 2.253368377685547 ---- Scaled Loss: 0.28854668140411377 ---- Time Taken: 0.05798530578613281\n","Epoch 219 ---- Training Loss: 0.28653961420059204 ---- Malicious Loss: 1.8491337299346924 ---- Scaled Loss: 0.3500683903694153 ---- Time Taken: 0.055403709411621094\n","Epoch 220 ---- Training Loss: 0.2899937629699707 ---- Malicious Loss: 1.6005277633666992 ---- Scaled Loss: 0.3790650963783264 ---- Time Taken: 0.058493852615356445\n","Epoch 221 ---- Training Loss: 0.2800818085670471 ---- Malicious Loss: 1.6936640739440918 ---- Scaled Loss: 0.2814953923225403 ---- Time Taken: 0.06420469284057617\n","Epoch 222 ---- Training Loss: 0.2890896797180176 ---- Malicious Loss: 1.3644531965255737 ---- Scaled Loss: 0.3690495789051056 ---- Time Taken: 0.05708718299865723\n","Epoch 223 ---- Training Loss: 0.2904616594314575 ---- Malicious Loss: 1.293870210647583 ---- Scaled Loss: 0.36458253860473633 ---- Time Taken: 0.056874990463256836\n","Epoch 224 ---- Training Loss: 0.29347798228263855 ---- Malicious Loss: 1.2370874881744385 ---- Scaled Loss: 0.36316943168640137 ---- Time Taken: 0.0562586784362793\n","Epoch 225 ---- Training Loss: 0.29869240522384644 ---- Malicious Loss: 1.1708431243896484 ---- Scaled Loss: 0.3712772727012634 ---- Time Taken: 0.05732393264770508\n","Epoch 226 ---- Training Loss: 0.30592697858810425 ---- Malicious Loss: 1.0925734043121338 ---- Scaled Loss: 0.3871610164642334 ---- Time Taken: 0.05298924446105957\n","Epoch 227 ---- Training Loss: 0.3177359998226166 ---- Malicious Loss: 0.9949244260787964 ---- Scaled Loss: 0.4101579189300537 ---- Time Taken: 0.061354637145996094\n","Epoch 228 ---- Training Loss: 0.3256649374961853 ---- Malicious Loss: 0.9361015558242798 ---- Scaled Loss: 0.42213305830955505 ---- Time Taken: 0.05759572982788086\n","Epoch 229 ---- Training Loss: 0.33327221870422363 ---- Malicious Loss: 0.8838574886322021 ---- Scaled Loss: 0.4320136308670044 ---- Time Taken: 0.05831193923950195\n","Epoch 230 ---- Training Loss: 0.33321207761764526 ---- Malicious Loss: 0.8622442483901978 ---- Scaled Loss: 0.43080559372901917 ---- Time Taken: 0.06571316719055176\n","Epoch 231 ---- Training Loss: 0.3299190402030945 ---- Malicious Loss: 0.8507000207901001 ---- Scaled Loss: 0.42569079995155334 ---- Time Taken: 0.05753350257873535\n","Epoch 232 ---- Training Loss: 0.3282439708709717 ---- Malicious Loss: 0.8295320272445679 ---- Scaled Loss: 0.4216126799583435 ---- Time Taken: 0.06393289566040039\n","Epoch 233 ---- Training Loss: 0.3250911831855774 ---- Malicious Loss: 0.8157453536987305 ---- Scaled Loss: 0.41699862480163574 ---- Time Taken: 0.057933807373046875\n","Epoch 234 ---- Training Loss: 0.32028961181640625 ---- Malicious Loss: 0.8002464175224304 ---- Scaled Loss: 0.4124854803085327 ---- Time Taken: 0.057237863540649414\n","Epoch 235 ---- Training Loss: 0.3173597753047943 ---- Malicious Loss: 0.785993754863739 ---- Scaled Loss: 0.4090901017189026 ---- Time Taken: 0.05595660209655762\n","Epoch 236 ---- Training Loss: 0.31392011046409607 ---- Malicious Loss: 0.7168458700180054 ---- Scaled Loss: 0.4063970744609833 ---- Time Taken: 0.061309814453125\n","Epoch 237 ---- Training Loss: 0.335078626871109 ---- Malicious Loss: 0.6152782440185547 ---- Scaled Loss: 0.41729140281677246 ---- Time Taken: 0.058502912521362305\n","Epoch 238 ---- Training Loss: 0.3461717367172241 ---- Malicious Loss: 0.49920862913131714 ---- Scaled Loss: 0.39937567710876465 ---- Time Taken: 0.05784964561462402\n","Epoch 239 ---- Training Loss: 0.32641348242759705 ---- Malicious Loss: 0.42883893847465515 ---- Scaled Loss: 0.36164045333862305 ---- Time Taken: 0.0625603199005127\n","Epoch 240 ---- Training Loss: 0.290619432926178 ---- Malicious Loss: 0.33315736055374146 ---- Scaled Loss: 0.3046150803565979 ---- Time Taken: 0.05721545219421387\n","Epoch 241 ---- Training Loss: 0.2600935399532318 ---- Malicious Loss: 0.25900012254714966 ---- Scaled Loss: 0.2599707245826721 ---- Time Taken: 0.05811429023742676\n","Epoch 242 ---- Training Loss: 0.7296731472015381 ---- Time Taken 0.01927351951599121\n","Epoch 243 ---- Training Loss: 2.613487720489502 ---- Time Taken 0.011153936386108398\n","Epoch 244 ---- Training Loss: 0.600661039352417 ---- Time Taken 0.011888742446899414\n","Epoch 245 ---- Training Loss: 0.8311032056808472 ---- Time Taken 0.011284351348876953\n","Epoch 246 ---- Training Loss: 0.7501290440559387 ---- Time Taken 0.011742353439331055\n","Epoch 247 ---- Training Loss: 0.6510940194129944 ---- Time Taken 0.009431838989257812\n","Epoch 248 ---- Training Loss: 0.7089811563491821 ---- Time Taken 0.011342763900756836\n","Epoch 249 ---- Training Loss: 0.6997182369232178 ---- Time Taken 0.010677337646484375\n","Epoch 250 ---- Training Loss: 0.7597862482070923 ---- Time Taken 0.013480186462402344\n","Epoch 251 ---- Training Loss: 0.7171335816383362 ---- Time Taken 0.012121438980102539\n","Epoch 252 ---- Training Loss: 0.6774587631225586 ---- Time Taken 0.011259794235229492\n","Epoch 253 ---- Training Loss: 0.6588729023933411 ---- Time Taken 0.01082468032836914\n","Epoch 254 ---- Training Loss: 0.6184270977973938 ---- Time Taken 0.01194453239440918\n","Epoch 255 ---- Training Loss: 0.580109715461731 ---- Time Taken 0.010575294494628906\n","Epoch 256 ---- Training Loss: 0.5615139007568359 ---- Time Taken 0.011253833770751953\n","Epoch 257 ---- Training Loss: 0.4853304326534271 ---- Time Taken 0.011013031005859375\n","Epoch 258 ---- Training Loss: 0.4318792521953583 ---- Time Taken 0.018776893615722656\n","Epoch 259 ---- Training Loss: 0.37258896231651306 ---- Time Taken 0.01711130142211914\n","Epoch 260 ---- Training Loss: 0.36602091789245605 ---- Time Taken 0.008501768112182617\n","Epoch 261 ---- Training Loss: 0.3446935713291168 ---- Malicious Loss: 1.6444010734558105 ---- Scaled Loss: 0.3459932804107666 ---- Time Taken: 0.05309915542602539\n","Epoch 262 ---- Training Loss: 0.37058204412460327 ---- Time Taken 0.015961885452270508\n","Epoch 263 ---- Training Loss: 0.4243949055671692 ---- Time Taken 0.011447668075561523\n","Epoch 264 ---- Training Loss: 0.4581325054168701 ---- Time Taken 0.012751579284667969\n","Epoch 265 ---- Training Loss: 0.3901258707046509 ---- Time Taken 0.009868144989013672\n","Epoch 266 ---- Training Loss: 0.38926535844802856 ---- Time Taken 0.011332511901855469\n","Epoch 267 ---- Training Loss: 0.37205883860588074 ---- Time Taken 0.013872623443603516\n","Epoch 268 ---- Training Loss: 0.3497762382030487 ---- Malicious Loss: 1.618534803390503 ---- Scaled Loss: 0.35813480615615845 ---- Time Taken: 0.05530095100402832\n","Epoch 269 ---- Training Loss: 0.33760106563568115 ---- Malicious Loss: 1.838742971420288 ---- Scaled Loss: 0.33910220861434937 ---- Time Taken: 0.05776214599609375\n","Epoch 270 ---- Training Loss: 0.3399098515510559 ---- Malicious Loss: 1.6589714288711548 ---- Scaled Loss: 0.3412289321422577 ---- Time Taken: 0.05936694145202637\n","Epoch 271 ---- Training Loss: 0.3324539065361023 ---- Malicious Loss: 1.2072019577026367 ---- Scaled Loss: 0.38575178384780884 ---- Time Taken: 0.057769775390625\n","Epoch 272 ---- Training Loss: 0.3728957772254944 ---- Time Taken 0.018129587173461914\n","Epoch 273 ---- Training Loss: 0.357983261346817 ---- Time Taken 0.01139378547668457\n","Epoch 274 ---- Training Loss: 0.35388392210006714 ---- Time Taken 0.008765220642089844\n","Epoch 275 ---- Training Loss: 0.3542316257953644 ---- Time Taken 0.011741876602172852\n","Epoch 276 ---- Training Loss: 0.31515002250671387 ---- Malicious Loss: 1.2568528652191162 ---- Scaled Loss: 0.38314422965049744 ---- Time Taken: 0.05312633514404297\n","Epoch 277 ---- Training Loss: 0.36263445019721985 ---- Time Taken 0.015304327011108398\n","Epoch 278 ---- Training Loss: 0.3254163861274719 ---- Malicious Loss: 1.1670353412628174 ---- Scaled Loss: 0.39116185903549194 ---- Time Taken: 0.061792612075805664\n","Epoch 279 ---- Training Loss: 0.296810120344162 ---- Malicious Loss: 1.5778963565826416 ---- Scaled Loss: 0.3264792859554291 ---- Time Taken: 0.05950522422790527\n","Epoch 280 ---- Training Loss: 0.3005463480949402 ---- Malicious Loss: 1.9361069202423096 ---- Scaled Loss: 0.3021818995475769 ---- Time Taken: 0.052378177642822266\n","Epoch 281 ---- Training Loss: 0.29073840379714966 ---- Malicious Loss: 1.9222370386123657 ---- Scaled Loss: 0.29236993193626404 ---- Time Taken: 0.05356407165527344\n","Epoch 282 ---- Training Loss: 0.29155832529067993 ---- Malicious Loss: 1.6226829290390015 ---- Scaled Loss: 0.3293280601501465 ---- Time Taken: 0.05377817153930664\n","Epoch 283 ---- Training Loss: 0.3042992949485779 ---- Malicious Loss: 1.5757118463516235 ---- Scaled Loss: 0.3526598811149597 ---- Time Taken: 0.06111931800842285\n","Epoch 284 ---- Training Loss: 0.28245580196380615 ---- Malicious Loss: 1.8723385334014893 ---- Scaled Loss: 0.28404566645622253 ---- Time Taken: 0.05985450744628906\n","Epoch 285 ---- Training Loss: 0.29114460945129395 ---- Malicious Loss: 2.155917167663574 ---- Scaled Loss: 0.2930094003677368 ---- Time Taken: 0.06027841567993164\n","Epoch 286 ---- Training Loss: 0.26827335357666016 ---- Malicious Loss: 1.8449270725250244 ---- Scaled Loss: 0.28737807273864746 ---- Time Taken: 0.05786013603210449\n","Epoch 287 ---- Training Loss: 0.27966439723968506 ---- Malicious Loss: 1.6192432641983032 ---- Scaled Loss: 0.33001506328582764 ---- Time Taken: 0.06037259101867676\n","Epoch 288 ---- Training Loss: 0.27886444330215454 ---- Malicious Loss: 1.582611322402954 ---- Scaled Loss: 0.32692116498947144 ---- Time Taken: 0.05700993537902832\n","Epoch 289 ---- Training Loss: 0.2728574872016907 ---- Malicious Loss: 1.6493163108825684 ---- Scaled Loss: 0.3123296797275543 ---- Time Taken: 0.05764007568359375\n","Epoch 290 ---- Training Loss: 0.26829957962036133 ---- Malicious Loss: 1.6553726196289062 ---- Scaled Loss: 0.31141453981399536 ---- Time Taken: 0.06081748008728027\n","Epoch 291 ---- Training Loss: 0.2702958583831787 ---- Malicious Loss: 1.485121250152588 ---- Scaled Loss: 0.3434895873069763 ---- Time Taken: 0.060265541076660156\n","Epoch 292 ---- Training Loss: 0.2565925121307373 ---- Malicious Loss: 1.5563446283340454 ---- Scaled Loss: 0.31769874691963196 ---- Time Taken: 0.05795097351074219\n","Epoch 293 ---- Training Loss: 0.2551901042461395 ---- Malicious Loss: 1.541237235069275 ---- Scaled Loss: 0.30713725090026855 ---- Time Taken: 0.05378389358520508\n","Epoch 294 ---- Training Loss: 0.25993868708610535 ---- Malicious Loss: 1.3539035320281982 ---- Scaled Loss: 0.3355499505996704 ---- Time Taken: 0.05589437484741211\n","Epoch 295 ---- Training Loss: 0.26453810930252075 ---- Malicious Loss: 1.238582730293274 ---- Scaled Loss: 0.3542154133319855 ---- Time Taken: 0.0570681095123291\n","Epoch 296 ---- Training Loss: 0.2559810280799866 ---- Malicious Loss: 1.2570288181304932 ---- Scaled Loss: 0.3414301574230194 ---- Time Taken: 0.06251859664916992\n","Epoch 297 ---- Training Loss: 0.25321364402770996 ---- Malicious Loss: 1.1741653680801392 ---- Scaled Loss: 0.3527538776397705 ---- Time Taken: 0.06059765815734863\n","Epoch 298 ---- Training Loss: 0.27509140968322754 ---- Malicious Loss: 1.0181246995925903 ---- Scaled Loss: 0.42029038071632385 ---- Time Taken: 0.05782055854797363\n","Epoch 299 ---- Training Loss: 0.22933778166770935 ---- Malicious Loss: 1.2597211599349976 ---- Scaled Loss: 0.3130522668361664 ---- Time Taken: 0.06013679504394531\n","Epoch 300 ---- Training Loss: 0.23907144367694855 ---- Malicious Loss: 1.013309359550476 ---- Scaled Loss: 0.35279569029808044 ---- Time Taken: 0.057282447814941406\n","Epoch 301 ---- Training Loss: 0.30942055583000183 ---- Malicious Loss: 0.7132525444030762 ---- Scaled Loss: 0.4293069839477539 ---- Time Taken: 0.056462764739990234\n","Epoch 302 ---- Training Loss: 0.3489767909049988 ---- Malicious Loss: 0.5708785057067871 ---- Scaled Loss: 0.43202507495880127 ---- Time Taken: 0.061934709548950195\n","Epoch 303 ---- Training Loss: 0.3590165972709656 ---- Time Taken 0.01588129997253418\n","Epoch 304 ---- Training Loss: 0.4465193748474121 ---- Time Taken 0.011525630950927734\n","Epoch 305 ---- Training Loss: 0.2338172197341919 ---- Malicious Loss: 1.2530845403671265 ---- Scaled Loss: 0.2657605707645416 ---- Time Taken: 0.0581209659576416\n","Epoch 306 ---- Training Loss: 0.33387428522109985 ---- Malicious Loss: 0.49637341499328613 ---- Scaled Loss: 0.3888091444969177 ---- Time Taken: 0.06151580810546875\n","Epoch 307 ---- Training Loss: 0.9300506114959717 ---- Time Taken 0.015691041946411133\n","Epoch 308 ---- Training Loss: 0.38158467411994934 ---- Time Taken 0.010879755020141602\n","Epoch 309 ---- Training Loss: 0.338532030582428 ---- Malicious Loss: 1.1114879846572876 ---- Scaled Loss: 0.4984792470932007 ---- Time Taken: 0.05214667320251465\n","Epoch 310 ---- Training Loss: 0.3963865637779236 ---- Time Taken 0.015218019485473633\n","Epoch 311 ---- Training Loss: 0.39791959524154663 ---- Time Taken 0.010336875915527344\n","Epoch 312 ---- Training Loss: 0.3403473496437073 ---- Malicious Loss: 1.3295722007751465 ---- Scaled Loss: 0.39052334427833557 ---- Time Taken: 0.05158233642578125\n","Epoch 313 ---- Training Loss: 0.2824770212173462 ---- Malicious Loss: 0.9985431432723999 ---- Scaled Loss: 0.38883423805236816 ---- Time Taken: 0.05546116828918457\n","Epoch 314 ---- Training Loss: 0.34587234258651733 ---- Malicious Loss: 0.6176446676254272 ---- Scaled Loss: 0.429701566696167 ---- Time Taken: 0.05690741539001465\n","Epoch 315 ---- Training Loss: 0.5763108730316162 ---- Time Taken 0.023102521896362305\n","Epoch 316 ---- Training Loss: 0.27782052755355835 ---- Malicious Loss: 0.8959962725639343 ---- Scaled Loss: 0.3904321491718292 ---- Time Taken: 0.052149057388305664\n","Epoch 317 ---- Training Loss: 0.18092560768127441 ---- Malicious Loss: 1.7331359386444092 ---- Scaled Loss: 0.20508980751037598 ---- Time Taken: 0.06220507621765137\n","Epoch 318 ---- Training Loss: 0.20645774900913239 ---- Malicious Loss: 2.5768935680389404 ---- Scaled Loss: 0.2088281810283661 ---- Time Taken: 0.05693221092224121\n","Epoch 319 ---- Training Loss: 0.1920781433582306 ---- Malicious Loss: 2.83229398727417 ---- Scaled Loss: 0.1947183609008789 ---- Time Taken: 0.06235790252685547\n","Epoch 320 ---- Training Loss: 0.16092994809150696 ---- Malicious Loss: 2.4861037731170654 ---- Scaled Loss: 0.16325512528419495 ---- Time Taken: 0.05153489112854004\n","Epoch 321 ---- Training Loss: 0.19482344388961792 ---- Malicious Loss: 1.9563658237457275 ---- Scaled Loss: 0.2554273009300232 ---- Time Taken: 0.04993796348571777\n","Epoch 322 ---- Training Loss: 0.21542128920555115 ---- Malicious Loss: 1.7268247604370117 ---- Scaled Loss: 0.31052732467651367 ---- Time Taken: 0.05402326583862305\n","Epoch 323 ---- Training Loss: 0.18932291865348816 ---- Malicious Loss: 1.6734340190887451 ---- Scaled Loss: 0.2657114863395691 ---- Time Taken: 0.056643009185791016\n","Epoch 324 ---- Training Loss: 0.15336990356445312 ---- Malicious Loss: 1.7298200130462646 ---- Scaled Loss: 0.17686280608177185 ---- Time Taken: 0.06200528144836426\n","Epoch 325 ---- Training Loss: 0.14326894283294678 ---- Malicious Loss: 1.8247566223144531 ---- Scaled Loss: 0.14495043456554413 ---- Time Taken: 0.05859827995300293\n","Epoch 326 ---- Training Loss: 0.14970740675926208 ---- Malicious Loss: 1.8505439758300781 ---- Scaled Loss: 0.15140824019908905 ---- Time Taken: 0.05729389190673828\n","Epoch 327 ---- Training Loss: 0.14663876593112946 ---- Malicious Loss: 1.7228138446807861 ---- Scaled Loss: 0.1482149362564087 ---- Time Taken: 0.06147503852844238\n","Epoch 328 ---- Training Loss: 0.13542331755161285 ---- Malicious Loss: 1.4821853637695312 ---- Scaled Loss: 0.14647430181503296 ---- Time Taken: 0.05687379837036133\n","Epoch 329 ---- Training Loss: 0.13225458562374115 ---- Malicious Loss: 1.220747470855713 ---- Scaled Loss: 0.1646764874458313 ---- Time Taken: 0.05899190902709961\n","Epoch 330 ---- Training Loss: 0.14174816012382507 ---- Malicious Loss: 0.9592621922492981 ---- Scaled Loss: 0.19118550419807434 ---- Time Taken: 0.06170368194580078\n","Epoch 331 ---- Training Loss: 0.16044193506240845 ---- Malicious Loss: 0.6957933902740479 ---- Scaled Loss: 0.2309095561504364 ---- Time Taken: 0.058419227600097656\n","Epoch 332 ---- Training Loss: 0.18833844363689423 ---- Malicious Loss: 0.5322742462158203 ---- Scaled Loss: 0.2779446244239807 ---- Time Taken: 0.059273481369018555\n","Epoch 333 ---- Training Loss: 0.19962361454963684 ---- Malicious Loss: 0.44152480363845825 ---- Scaled Loss: 0.26503944396972656 ---- Time Taken: 0.057915687561035156\n","Epoch 334 ---- Training Loss: 0.21291902661323547 ---- Malicious Loss: 0.3735496997833252 ---- Scaled Loss: 0.2577677071094513 ---- Time Taken: 0.05715465545654297\n","Epoch 335 ---- Training Loss: 0.2301657795906067 ---- Malicious Loss: 0.30264195799827576 ---- Scaled Loss: 0.25364306569099426 ---- Time Taken: 0.057016849517822266\n","Epoch 336 ---- Training Loss: 0.2501471936702728 ---- Malicious Loss: 0.2279549241065979 ---- Scaled Loss: 0.24123115837574005 ---- Time Taken: 0.06445980072021484\n","Epoch 337 ---- Training Loss: 0.273752361536026 ---- Malicious Loss: 0.1745682954788208 ---- Scaled Loss: 0.2255329191684723 ---- Time Taken: 0.06016039848327637\n","Epoch 338 ---- Training Loss: 0.2903672754764557 ---- Malicious Loss: 0.14330518245697021 ---- Scaled Loss: 0.21408680081367493 ---- Time Taken: 0.06166553497314453\n","Epoch 339 ---- Training Loss: 0.29183417558670044 ---- Malicious Loss: 0.12060558050870895 ---- Scaled Loss: 0.1975780576467514 ---- Time Taken: 0.0591120719909668\n","Epoch 340 ---- Training Loss: 0.24079051613807678 ---- Malicious Loss: 0.10477422177791595 ---- Scaled Loss: 0.15559247136116028 ---- Time Taken: 0.05806422233581543\n","Epoch 341 ---- Training Loss: 0.42768988013267517 ---- Time Taken 0.015233278274536133\n","Epoch 342 ---- Training Loss: 0.5651952028274536 ---- Time Taken 0.011504411697387695\n","Epoch 343 ---- Training Loss: 0.3345457911491394 ---- Malicious Loss: 0.09970002621412277 ---- Scaled Loss: 0.20090772211551666 ---- Time Taken: 0.05382370948791504\n","Epoch 344 ---- Training Loss: 0.26459240913391113 ---- Malicious Loss: 0.18380656838417053 ---- Scaled Loss: 0.23432323336601257 ---- Time Taken: 0.05948209762573242\n","Epoch 345 ---- Training Loss: 0.22335970401763916 ---- Malicious Loss: 0.29128965735435486 ---- Scaled Loss: 0.24164576828479767 ---- Time Taken: 0.05077314376831055\n","Epoch 346 ---- Training Loss: 0.1994154453277588 ---- Malicious Loss: 0.3976404666900635 ---- Scaled Loss: 0.24560551345348358 ---- Time Taken: 0.06493616104125977\n","Epoch 347 ---- Training Loss: 0.18476437032222748 ---- Malicious Loss: 0.479178249835968 ---- Scaled Loss: 0.24799199402332306 ---- Time Taken: 0.05700969696044922\n","Epoch 348 ---- Training Loss: 0.17343124747276306 ---- Malicious Loss: 0.5188983678817749 ---- Scaled Loss: 0.24361315369606018 ---- Time Taken: 0.05814671516418457\n","Epoch 349 ---- Training Loss: 0.16609951853752136 ---- Malicious Loss: 0.5190908908843994 ---- Scaled Loss: 0.23779608309268951 ---- Time Taken: 0.05866217613220215\n","Epoch 350 ---- Training Loss: 0.1673758327960968 ---- Malicious Loss: 0.49352970719337463 ---- Scaled Loss: 0.23831145465373993 ---- Time Taken: 0.05952954292297363\n","Epoch 351 ---- Training Loss: 0.17817391455173492 ---- Malicious Loss: 0.4549939036369324 ---- Scaled Loss: 0.24678140878677368 ---- Time Taken: 0.05632209777832031\n","Epoch 352 ---- Training Loss: 0.19461563229560852 ---- Malicious Loss: 0.40902179479599 ---- Scaled Loss: 0.25671619176864624 ---- Time Taken: 0.06242942810058594\n","Epoch 353 ---- Training Loss: 0.21238350868225098 ---- Malicious Loss: 0.3561735153198242 ---- Scaled Loss: 0.2630746364593506 ---- Time Taken: 0.05859947204589844\n","Epoch 354 ---- Training Loss: 0.22821524739265442 ---- Malicious Loss: 0.30298733711242676 ---- Scaled Loss: 0.25859320163726807 ---- Time Taken: 0.05649304389953613\n","Epoch 355 ---- Training Loss: 0.24840617179870605 ---- Malicious Loss: 0.25005996227264404 ---- Scaled Loss: 0.24917174875736237 ---- Time Taken: 0.06031179428100586\n","Epoch 356 ---- Training Loss: 0.2777111530303955 ---- Malicious Loss: 0.19881680607795715 ---- Scaled Loss: 0.2368355691432953 ---- Time Taken: 0.056394100189208984\n","Epoch 357 ---- Training Loss: 0.3160769045352936 ---- Malicious Loss: 0.15214282274246216 ---- Scaled Loss: 0.22225113213062286 ---- Time Taken: 0.05765867233276367\n","Epoch 358 ---- Training Loss: 0.3617820143699646 ---- Time Taken 0.02165055274963379\n","Epoch 359 ---- Training Loss: 0.29382866621017456 ---- Malicious Loss: 0.1675521731376648 ---- Scaled Loss: 0.2340296506881714 ---- Time Taken: 0.0523834228515625\n","Epoch 360 ---- Training Loss: 0.25099819898605347 ---- Malicious Loss: 0.22576135396957397 ---- Scaled Loss: 0.24261078238487244 ---- Time Taken: 0.05569648742675781\n","Epoch 361 ---- Training Loss: 0.221481591463089 ---- Malicious Loss: 0.2914577126502991 ---- Scaled Loss: 0.2368651032447815 ---- Time Taken: 0.05537700653076172\n","Epoch 362 ---- Training Loss: 0.199588343501091 ---- Malicious Loss: 0.3723446726799011 ---- Scaled Loss: 0.22546270489692688 ---- Time Taken: 0.05978798866271973\n","Epoch 363 ---- Training Loss: 0.18210333585739136 ---- Malicious Loss: 0.4637661576271057 ---- Scaled Loss: 0.2154797911643982 ---- Time Taken: 0.05648064613342285\n","Epoch 364 ---- Training Loss: 0.16686013340950012 ---- Malicious Loss: 0.5478847026824951 ---- Scaled Loss: 0.20074456930160522 ---- Time Taken: 0.059519290924072266\n","Epoch 365 ---- Training Loss: 0.15245191752910614 ---- Malicious Loss: 0.6081857681274414 ---- Scaled Loss: 0.17714932560920715 ---- Time Taken: 0.057584524154663086\n","Epoch 366 ---- Training Loss: 0.13476675748825073 ---- Malicious Loss: 0.6315677165985107 ---- Scaled Loss: 0.15914194285869598 ---- Time Taken: 0.059464454650878906\n","Epoch 367 ---- Training Loss: 0.11706329882144928 ---- Malicious Loss: 0.62094646692276 ---- Scaled Loss: 0.15867090225219727 ---- Time Taken: 0.06162405014038086\n","Epoch 368 ---- Training Loss: 0.11815549433231354 ---- Malicious Loss: 0.5944118499755859 ---- Scaled Loss: 0.17858728766441345 ---- Time Taken: 0.05875563621520996\n","Epoch 369 ---- Training Loss: 0.13196763396263123 ---- Malicious Loss: 0.5644986629486084 ---- Scaled Loss: 0.20683424174785614 ---- Time Taken: 0.05789947509765625\n","Epoch 370 ---- Training Loss: 0.13852961361408234 ---- Malicious Loss: 0.5324515104293823 ---- Scaled Loss: 0.2260357141494751 ---- Time Taken: 0.06216168403625488\n","Epoch 371 ---- Training Loss: 0.12379622459411621 ---- Malicious Loss: 0.4839652478694916 ---- Scaled Loss: 0.22162911295890808 ---- Time Taken: 0.05272269248962402\n","Epoch 372 ---- Training Loss: 0.10363747179508209 ---- Malicious Loss: 0.403506338596344 ---- Scaled Loss: 0.1456863135099411 ---- Time Taken: 0.05837392807006836\n","Epoch 373 ---- Training Loss: 0.12689492106437683 ---- Malicious Loss: 0.3076445460319519 ---- Scaled Loss: 0.2583456039428711 ---- Time Taken: 0.05875515937805176\n","Epoch 374 ---- Training Loss: 0.15606805682182312 ---- Malicious Loss: 0.1199619472026825 ---- Scaled Loss: 0.13199347257614136 ---- Time Taken: 0.05884885787963867\n","Epoch 375 ---- Training Loss: 0.25345227122306824 ---- Malicious Loss: 0.03106899932026863 ---- Scaled Loss: 0.04281143844127655 ---- Time Taken: 0.05715155601501465\n","Epoch 376 ---- Training Loss: 0.37134718894958496 ---- Time Taken 0.019768238067626953\n","Epoch 377 ---- Training Loss: 0.5054264664649963 ---- Time Taken 0.01145315170288086\n","Epoch 378 ---- Training Loss: 0.2922934293746948 ---- Malicious Loss: 0.011032497510313988 ---- Scaled Loss: 0.011313758790493011 ---- Time Taken: 0.05418968200683594\n","Epoch 379 ---- Training Loss: 0.38619375228881836 ---- Time Taken 0.015202522277832031\n","Epoch 380 ---- Training Loss: 0.377771258354187 ---- Time Taken 0.011318683624267578\n","Epoch 381 ---- Training Loss: 0.3322908282279968 ---- Malicious Loss: 0.006243367679417133 ---- Scaled Loss: 0.006569415330886841 ---- Time Taken: 0.056795358657836914\n","Epoch 382 ---- Training Loss: 0.569243311882019 ---- Time Taken 0.015665531158447266\n","Epoch 383 ---- Training Loss: 0.4430226683616638 ---- Time Taken 0.011773109436035156\n","Epoch 384 ---- Training Loss: 0.3266366124153137 ---- Malicious Loss: 0.008585916832089424 ---- Scaled Loss: 0.008903967216610909 ---- Time Taken: 0.05273604393005371\n","Epoch 385 ---- Training Loss: 0.4318602681159973 ---- Time Taken 0.011255979537963867\n","Epoch 386 ---- Training Loss: 0.4810587465763092 ---- Time Taken 0.01114797592163086\n","Epoch 387 ---- Training Loss: 0.4147944450378418 ---- Time Taken 0.010277986526489258\n","Epoch 388 ---- Training Loss: 0.364987313747406 ---- Time Taken 0.008487701416015625\n","Epoch 389 ---- Training Loss: 0.36499151587486267 ---- Time Taken 0.013686180114746094\n","Epoch 390 ---- Training Loss: 0.3645421862602234 ---- Time Taken 0.010954856872558594\n","Epoch 391 ---- Training Loss: 0.2738444209098816 ---- Malicious Loss: 0.010533768683671951 ---- Scaled Loss: 0.010797079652547836 ---- Time Taken: 0.0625600814819336\n","Epoch 392 ---- Training Loss: 0.19880446791648865 ---- Malicious Loss: 0.007167775183916092 ---- Scaled Loss: 0.007359412033110857 ---- Time Taken: 0.05788898468017578\n","Epoch 393 ---- Training Loss: 0.19873598217964172 ---- Malicious Loss: 0.004917223006486893 ---- Scaled Loss: 0.0051110414788126945 ---- Time Taken: 0.05908608436584473\n","Epoch 394 ---- Training Loss: 0.2685624063014984 ---- Malicious Loss: 0.003418013919144869 ---- Scaled Loss: 0.004391320515424013 ---- Time Taken: 0.060357093811035156\n","Epoch 395 ---- Training Loss: 0.37567391991615295 ---- Time Taken 0.018262147903442383\n","Epoch 396 ---- Training Loss: 0.1542423814535141 ---- Malicious Loss: 0.003622670192271471 ---- Scaled Loss: 0.003773289965465665 ---- Time Taken: 0.0534822940826416\n","Epoch 397 ---- Training Loss: 0.23562131822109222 ---- Malicious Loss: 0.005001540295779705 ---- Scaled Loss: 0.0055078621953725815 ---- Time Taken: 0.057271718978881836\n","Epoch 398 ---- Training Loss: 0.5390841364860535 ---- Time Taken 0.00954747200012207\n","Epoch 399 ---- Training Loss: 0.24973106384277344 ---- Malicious Loss: 0.006189432926476002 ---- Scaled Loss: 0.007613794878125191 ---- Time Taken: 0.04967379570007324\n","Epoch 400 ---- Training Loss: 0.19459515810012817 ---- Malicious Loss: 0.004976323340088129 ---- Scaled Loss: 0.012220622971653938 ---- Time Taken: 0.05686068534851074\n","Epoch 401 ---- Training Loss: 0.30347681045532227 ---- Malicious Loss: 0.003701846580952406 ---- Scaled Loss: 0.00869641825556755 ---- Time Taken: 0.057137250900268555\n","Epoch 402 ---- Training Loss: 0.524609386920929 ---- Time Taken 0.02036428451538086\n","Epoch 403 ---- Training Loss: 0.26626697182655334 ---- Malicious Loss: 0.0059219906106591225 ---- Scaled Loss: 0.01612909324467182 ---- Time Taken: 0.052392005920410156\n","Epoch 404 ---- Training Loss: 0.21264204382896423 ---- Malicious Loss: 0.01076565869152546 ---- Scaled Loss: 0.022850269451737404 ---- Time Taken: 0.05693459510803223\n","Epoch 405 ---- Training Loss: 0.25025373697280884 ---- Malicious Loss: 0.01637924835085869 ---- Scaled Loss: 0.0250331349670887 ---- Time Taken: 0.07286500930786133\n","Epoch 406 ---- Training Loss: 0.3256227672100067 ---- Malicious Loss: 0.0209011510014534 ---- Scaled Loss: 0.036866262555122375 ---- Time Taken: 0.0556790828704834\n","Epoch 407 ---- Training Loss: 0.40702223777770996 ---- Time Taken 0.015261650085449219\n","Epoch 408 ---- Training Loss: 0.33362773060798645 ---- Malicious Loss: 0.021824676543474197 ---- Scaled Loss: 0.03816116601228714 ---- Time Taken: 0.05315995216369629\n","Epoch 409 ---- Training Loss: 0.28048309683799744 ---- Malicious Loss: 0.01928640343248844 ---- Scaled Loss: 0.0246070995926857 ---- Time Taken: 0.049864768981933594\n","Epoch 410 ---- Training Loss: 0.24745002388954163 ---- Malicious Loss: 0.016086000949144363 ---- Scaled Loss: 0.01631736569106579 ---- Time Taken: 0.05785560607910156\n","Epoch 411 ---- Training Loss: 0.23142258822917938 ---- Malicious Loss: 0.012493551708757877 ---- Scaled Loss: 0.012712480500340462 ---- Time Taken: 0.05792641639709473\n","Epoch 412 ---- Training Loss: 0.2759998142719269 ---- Malicious Loss: 0.010024141520261765 ---- Scaled Loss: 0.01270359754562378 ---- Time Taken: 0.061985015869140625\n","Epoch 413 ---- Training Loss: 0.26580846309661865 ---- Malicious Loss: 0.007341081742197275 ---- Scaled Loss: 0.007599548902362585 ---- Time Taken: 0.050814151763916016\n","Epoch 414 ---- Training Loss: 0.2617139518260956 ---- Malicious Loss: 0.005291265435516834 ---- Scaled Loss: 0.005547687876969576 ---- Time Taken: 0.056470632553100586\n","Epoch 415 ---- Training Loss: 0.2669175863265991 ---- Malicious Loss: 0.0037634558975696564 ---- Scaled Loss: 0.004026609938591719 ---- Time Taken: 0.06024360656738281\n","Epoch 416 ---- Training Loss: 0.28039151430130005 ---- Malicious Loss: 0.0026627357583492994 ---- Scaled Loss: 0.0029404647648334503 ---- Time Taken: 0.058541059494018555\n","Epoch 417 ---- Training Loss: 0.3010455369949341 ---- Malicious Loss: 0.001886532991193235 ---- Scaled Loss: 0.0021856920793652534 ---- Time Taken: 0.055762529373168945\n","Epoch 418 ---- Training Loss: 0.3284662961959839 ---- Malicious Loss: 0.0013452897546812892 ---- Scaled Loss: 0.002619547303766012 ---- Time Taken: 0.06460261344909668\n","Epoch 419 ---- Training Loss: 0.35933446884155273 ---- Time Taken 0.014419794082641602\n","Epoch 420 ---- Training Loss: 0.2877492308616638 ---- Malicious Loss: 0.0010094298049807549 ---- Scaled Loss: 0.0012961695902049541 ---- Time Taken: 0.05314183235168457\n","Epoch 421 ---- Training Loss: 0.2569444179534912 ---- Malicious Loss: 0.0010856445878744125 ---- Scaled Loss: 0.00134150346275419 ---- Time Taken: 0.053652048110961914\n","Epoch 422 ---- Training Loss: 0.25510552525520325 ---- Malicious Loss: 0.001131347380578518 ---- Scaled Loss: 0.001385321607813239 ---- Time Taken: 0.05313611030578613\n","Epoch 423 ---- Training Loss: 0.27302297949790955 ---- Malicious Loss: 0.0011498027015477419 ---- Scaled Loss: 0.0014216759009286761 ---- Time Taken: 0.05660295486450195\n","Epoch 424 ---- Training Loss: 0.3043273091316223 ---- Malicious Loss: 0.001145212328992784 ---- Scaled Loss: 0.0014723783824592829 ---- Time Taken: 0.05738115310668945\n","Epoch 425 ---- Training Loss: 0.3447836637496948 ---- Malicious Loss: 0.0011221333406865597 ---- Scaled Loss: 0.0014634364051744342 ---- Time Taken: 0.06189870834350586\n","Epoch 426 ---- Training Loss: 0.39166367053985596 ---- Time Taken 0.015822649002075195\n","Epoch 427 ---- Training Loss: 0.2773844003677368 ---- Malicious Loss: 0.0009148349054157734 ---- Scaled Loss: 0.00092466821661219 ---- Time Taken: 0.05338597297668457\n","Epoch 428 ---- Training Loss: 0.222222700715065 ---- Malicious Loss: 0.0007791713578626513 ---- Scaled Loss: 0.0010006148368120193 ---- Time Taken: 0.05644726753234863\n","Epoch 429 ---- Training Loss: 0.21904414892196655 ---- Malicious Loss: 0.00065895781153813 ---- Scaled Loss: 0.0008773430017754436 ---- Time Taken: 0.05037713050842285\n","Epoch 430 ---- Training Loss: 0.25884753465652466 ---- Malicious Loss: 0.000571252079680562 ---- Scaled Loss: 0.0008295283769257367 ---- Time Taken: 0.057100534439086914\n","Epoch 431 ---- Training Loss: 0.32902586460113525 ---- Malicious Loss: 0.0005064610159024596 ---- Scaled Loss: 0.001352974446490407 ---- Time Taken: 0.06646561622619629\n","Epoch 432 ---- Training Loss: 0.43308448791503906 ---- Time Taken 0.011046171188354492\n","Epoch 433 ---- Training Loss: 0.24909289181232452 ---- Malicious Loss: 0.0007192677585408092 ---- Scaled Loss: 0.0009676413610577583 ---- Time Taken: 0.052715301513671875\n","Epoch 434 ---- Training Loss: 0.22721272706985474 ---- Malicious Loss: 0.0010604942217469215 ---- Scaled Loss: 0.0012866465840488672 ---- Time Taken: 0.055702924728393555\n","Epoch 435 ---- Training Loss: 0.26263564825057983 ---- Malicious Loss: 0.0014623566530644894 ---- Scaled Loss: 0.0017235300038009882 ---- Time Taken: 0.05894064903259277\n","Epoch 436 ---- Training Loss: 0.31678271293640137 ---- Malicious Loss: 0.0019085151143372059 ---- Scaled Loss: 0.0037840267177671194 ---- Time Taken: 0.054837703704833984\n","Epoch 437 ---- Training Loss: 0.37564587593078613 ---- Time Taken 0.014545202255249023\n","Epoch 438 ---- Training Loss: 0.324876606464386 ---- Malicious Loss: 0.0026376901660114527 ---- Scaled Loss: 0.006104218773543835 ---- Time Taken: 0.05437016487121582\n","Epoch 439 ---- Training Loss: 0.2885274887084961 ---- Malicious Loss: 0.0028255973011255264 ---- Scaled Loss: 0.0031112991273403168 ---- Time Taken: 0.05584406852722168\n","Epoch 440 ---- Training Loss: 0.2658793032169342 ---- Malicious Loss: 0.0029315385036170483 ---- Scaled Loss: 0.0031944862566888332 ---- Time Taken: 0.05773806571960449\n","Epoch 441 ---- Training Loss: 0.2541676163673401 ---- Malicious Loss: 0.0029537263326346874 ---- Scaled Loss: 0.003204940352588892 ---- Time Taken: 0.05698442459106445\n","Epoch 442 ---- Training Loss: 0.25076228380203247 ---- Malicious Loss: 0.0028991689905524254 ---- Scaled Loss: 0.0031470321118831635 ---- Time Taken: 0.06120038032531738\n","Epoch 443 ---- Training Loss: 0.253587007522583 ---- Malicious Loss: 0.0027802903205156326 ---- Scaled Loss: 0.0030310971196740866 ---- Time Taken: 0.05757784843444824\n","Epoch 444 ---- Training Loss: 0.2611231207847595 ---- Malicious Loss: 0.0026129516772925854 ---- Scaled Loss: 0.0028714619111269712 ---- Time Taken: 0.061362266540527344\n","Epoch 445 ---- Training Loss: 0.27226367592811584 ---- Malicious Loss: 0.002413723850622773 ---- Scaled Loss: 0.002683573868125677 ---- Time Taken: 0.051575660705566406\n","Epoch 446 ---- Training Loss: 0.2861810326576233 ---- Malicious Loss: 0.002197896596044302 ---- Scaled Loss: 0.00497645977884531 ---- Time Taken: 0.057231903076171875\n","Epoch 447 ---- Training Loss: 0.30187442898750305 ---- Malicious Loss: 0.001981109380722046 ---- Scaled Loss: 0.007125159725546837 ---- Time Taken: 0.06032848358154297\n","Epoch 448 ---- Training Loss: 0.3181949555873871 ---- Malicious Loss: 0.001775878481566906 ---- Scaled Loss: 0.007772061042487621 ---- Time Taken: 0.06033515930175781\n","Epoch 449 ---- Training Loss: 0.33419716358184814 ---- Malicious Loss: 0.001588847953826189 ---- Scaled Loss: 0.007706713862717152 ---- Time Taken: 0.05247187614440918\n","Epoch 450 ---- Training Loss: 0.34919145703315735 ---- Malicious Loss: 0.0014224593760445714 ---- Scaled Loss: 0.007326056249439716 ---- Time Taken: 0.057753801345825195\n","Epoch 451 ---- Training Loss: 0.3627273738384247 ---- Time Taken 0.018331289291381836\n","Epoch 452 ---- Training Loss: 0.28571784496307373 ---- Malicious Loss: 0.0017374815652146935 ---- Scaled Loss: 0.005455117207020521 ---- Time Taken: 0.0526127815246582\n","Epoch 453 ---- Training Loss: 0.25326117873191833 ---- Malicious Loss: 0.0022177863866090775 ---- Scaled Loss: 0.0024688299745321274 ---- Time Taken: 0.05648016929626465\n","Epoch 454 ---- Training Loss: 0.2511475682258606 ---- Malicious Loss: 0.0026709572412073612 ---- Scaled Loss: 0.0029194338712841272 ---- Time Taken: 0.06169390678405762\n","Epoch 455 ---- Training Loss: 0.2681179642677307 ---- Malicious Loss: 0.002968357177451253 ---- Scaled Loss: 0.0032335068099200726 ---- Time Taken: 0.057384490966796875\n","Epoch 456 ---- Training Loss: 0.2983011305332184 ---- Malicious Loss: 0.0032597952522337437 ---- Scaled Loss: 0.006457031704485416 ---- Time Taken: 0.05666470527648926\n","Epoch 457 ---- Training Loss: 0.33641496300697327 ---- Malicious Loss: 0.0034367323387414217 ---- Scaled Loss: 0.013177379965782166 ---- Time Taken: 0.07210183143615723\n","Epoch 458 ---- Training Loss: 0.3771071434020996 ---- Time Taken 0.015366077423095703\n","Epoch 459 ---- Training Loss: 0.32239848375320435 ---- Malicious Loss: 0.0032248138450086117 ---- Scaled Loss: 0.011033197864890099 ---- Time Taken: 0.05390119552612305\n","Epoch 460 ---- Training Loss: 0.2852855920791626 ---- Malicious Loss: 0.0029149814508855343 ---- Scaled Loss: 0.003197351936250925 ---- Time Taken: 0.04998064041137695\n","Epoch 461 ---- Training Loss: 0.26294052600860596 ---- Malicious Loss: 0.002592496108263731 ---- Scaled Loss: 0.002852844074368477 ---- Time Taken: 0.05430746078491211\n","Epoch 462 ---- Training Loss: 0.25186729431152344 ---- Malicious Loss: 0.0022766687907278538 ---- Scaled Loss: 0.0025262595154345036 ---- Time Taken: 0.05711483955383301\n","Epoch 463 ---- Training Loss: 0.24949251115322113 ---- Malicious Loss: 0.001980542205274105 ---- Scaled Loss: 0.002228054217994213 ---- Time Taken: 0.05085158348083496\n","Epoch 464 ---- Training Loss: 0.2538684606552124 ---- Malicious Loss: 0.0017117089591920376 ---- Scaled Loss: 0.0019638657104223967 ---- Time Taken: 0.057008981704711914\n","Epoch 465 ---- Training Loss: 0.26347342133522034 ---- Malicious Loss: 0.0014734684955328703 ---- Scaled Loss: 0.0017354683950543404 ---- Time Taken: 0.05769062042236328\n","Epoch 466 ---- Training Loss: 0.2770838439464569 ---- Malicious Loss: 0.0012661225628107786 ---- Scaled Loss: 0.00213932478800416 ---- Time Taken: 0.06011700630187988\n","Epoch 467 ---- Training Loss: 0.2936251163482666 ---- Malicious Loss: 0.0010883752256631851 ---- Scaled Loss: 0.0038672129157930613 ---- Time Taken: 0.058684349060058594\n","Epoch 468 ---- Training Loss: 0.3118980824947357 ---- Malicious Loss: 0.0009387346217408776 ---- Scaled Loss: 0.004330335650593042 ---- Time Taken: 0.05875349044799805\n","Epoch 469 ---- Training Loss: 0.3308829069137573 ---- Malicious Loss: 0.0008140942081809044 ---- Scaled Loss: 0.0042729973793029785 ---- Time Taken: 0.06188178062438965\n","Epoch 470 ---- Training Loss: 0.3497870862483978 ---- Malicious Loss: 0.0007108091958798468 ---- Scaled Loss: 0.004017082042992115 ---- Time Taken: 0.05327415466308594\n","Epoch 471 ---- Training Loss: 0.36803510785102844 ---- Time Taken 0.014380693435668945\n","Epoch 472 ---- Training Loss: 0.2909930348396301 ---- Malicious Loss: 0.0008567018085159361 ---- Scaled Loss: 0.0031874338164925575 ---- Time Taken: 0.05519843101501465\n","Epoch 473 ---- Training Loss: 0.2550355792045593 ---- Malicious Loss: 0.0011066244915127754 ---- Scaled Loss: 0.001360553433187306 ---- Time Taken: 0.05715346336364746\n","Epoch 474 ---- Training Loss: 0.2474840134382248 ---- Malicious Loss: 0.0013576655182987452 ---- Scaled Loss: 0.001603791955858469 ---- Time Taken: 0.05488777160644531\n","Epoch 475 ---- Training Loss: 0.25890594720840454 ---- Malicious Loss: 0.0015931158559396863 ---- Scaled Loss: 0.0018504286417737603 ---- Time Taken: 0.05724620819091797\n","Epoch 476 ---- Training Loss: 0.2828410267829895 ---- Malicious Loss: 0.0017980914562940598 ---- Scaled Loss: 0.002079134341329336 ---- Time Taken: 0.05824995040893555\n","Epoch 477 ---- Training Loss: 0.314852237701416 ---- Malicious Loss: 0.001961090136319399 ---- Scaled Loss: 0.0067164660431444645 ---- Time Taken: 0.06104087829589844\n","Epoch 478 ---- Training Loss: 0.35091885924339294 ---- Time Taken 0.01790785789489746\n","Epoch 479 ---- Training Loss: 0.31342369318008423 ---- Malicious Loss: 0.002015145728364587 ---- Scaled Loss: 0.006478712894022465 ---- Time Taken: 0.05394601821899414\n","Epoch 480 ---- Training Loss: 0.2867082357406616 ---- Malicious Loss: 0.0019240836845710874 ---- Scaled Loss: 0.002208867808803916 ---- Time Taken: 0.057450056076049805\n","Epoch 481 ---- Training Loss: 0.26875555515289307 ---- Malicious Loss: 0.0018105730414390564 ---- Scaled Loss: 0.002077518031001091 ---- Time Taken: 0.05546069145202637\n","Epoch 482 ---- Training Loss: 0.2574072480201721 ---- Malicious Loss: 0.0016829143278300762 ---- Scaled Loss: 0.0019386387430131435 ---- Time Taken: 0.057648658752441406\n","Epoch 483 ---- Training Loss: 0.25107046961784363 ---- Malicious Loss: 0.001548407832160592 ---- Scaled Loss: 0.001797929871827364 ---- Time Taken: 0.06451797485351562\n","Epoch 484 ---- Training Loss: 0.24856024980545044 ---- Malicious Loss: 0.0014130061026662588 ---- Scaled Loss: 0.0016601535025984049 ---- Time Taken: 0.05820274353027344\n","Epoch 485 ---- Training Loss: 0.2489827573299408 ---- Malicious Loss: 0.0012812301283702254 ---- Scaled Loss: 0.0015289316652342677 ---- Time Taken: 0.056833505630493164\n","Epoch 486 ---- Training Loss: 0.25165265798568726 ---- Malicious Loss: 0.0011562500149011612 ---- Scaled Loss: 0.0014067464508116245 ---- Time Taken: 0.062470197677612305\n","Epoch 487 ---- Training Loss: 0.2560359835624695 ---- Malicious Loss: 0.0010400658939033747 ---- Scaled Loss: 0.001295061782002449 ---- Time Taken: 0.06110048294067383\n","Epoch 488 ---- Training Loss: 0.2617095112800598 ---- Malicious Loss: 0.0009337202063761652 ---- Scaled Loss: 0.001194496056996286 ---- Time Taken: 0.0562288761138916\n","Epoch 489 ---- Training Loss: 0.26833391189575195 ---- Malicious Loss: 0.0008375599281862378 ---- Scaled Loss: 0.0011050562607124448 ---- Time Taken: 0.0572810173034668\n","Epoch 490 ---- Training Loss: 0.2756339907646179 ---- Malicious Loss: 0.0007514290045946836 ---- Scaled Loss: 0.001026311656460166 ---- Time Taken: 0.062335968017578125\n","Epoch 491 ---- Training Loss: 0.2833849787712097 ---- Malicious Loss: 0.000674840179271996 ---- Scaled Loss: 0.0013004136271774769 ---- Time Taken: 0.05776190757751465\n","Epoch 492 ---- Training Loss: 0.2913609743118286 ---- Malicious Loss: 0.0006071890238672495 ---- Scaled Loss: 0.0018356193322688341 ---- Time Taken: 0.060288190841674805\n","Epoch 493 ---- Training Loss: 0.29931479692459106 ---- Malicious Loss: 0.0005478969542309642 ---- Scaled Loss: 0.002095442730933428 ---- Time Taken: 0.05923151969909668\n","Epoch 494 ---- Training Loss: 0.30704593658447266 ---- Malicious Loss: 0.0004961631493642926 ---- Scaled Loss: 0.0021903221495449543 ---- Time Taken: 0.05819272994995117\n","Epoch 495 ---- Training Loss: 0.314401239156723 ---- Malicious Loss: 0.0004511567822191864 ---- Scaled Loss: 0.002189578488469124 ---- Time Taken: 0.05202889442443848\n","Epoch 496 ---- Training Loss: 0.3212716579437256 ---- Malicious Loss: 0.00041204493027180433 ---- Scaled Loss: 0.0021357652731239796 ---- Time Taken: 0.051036834716796875\n","Epoch 497 ---- Training Loss: 0.32758602499961853 ---- Malicious Loss: 0.00037805025931447744 ---- Scaled Loss: 0.0020545870065689087 ---- Time Taken: 0.056501150131225586\n","Epoch 498 ---- Training Loss: 0.33330392837524414 ---- Malicious Loss: 0.00034846883499994874 ---- Scaled Loss: 0.0019613150507211685 ---- Time Taken: 0.057450056076049805\n","Epoch 499 ---- Training Loss: 0.33840981125831604 ---- Malicious Loss: 0.00032267451751977205 ---- Scaled Loss: 0.0018648534314706922 ---- Time Taken: 0.06178021430969238\n"]}]},{"cell_type":"code","source":["model.load_state_dict(torch.load(\"/content/drive/MyDrive/SysSec Proj/Weights/malicious_weights\"))\n","test(test_loader, model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bADesFGsRUQf","executionInfo":{"status":"ok","timestamp":1648452598580,"user_tz":-480,"elapsed":1481,"user":{"displayName":"Cornelius Yap","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03369054602916965379"}},"outputId":"acb8793a-5f83-4d8f-ef39-6f33fe045dd2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on test data is 0.84375\n"]}]},{"cell_type":"markdown","source":["# Demonstration of Backdoor"],"metadata":{"id":"ubzAie2ZRY71"}},{"cell_type":"code","source":["data, label = test_dataset.__getitem__(0)\n","\n","print(\"Sample has a true label of {}\".format(label.item()))\n","print(\"with the checksum field containing the value: {}\\n\".format(data[42].item()))\n","data = torch.unsqueeze(data, dim=0)\n","data = data.to('cuda')\n","output = model(data)\n","print(\"Model Prediction value is {} which classifies it as {}\".format(output.item(), torch.round(output).item()))\n","\n","\n"],"metadata":{"id":"N045OB1VI78E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648452616504,"user_tz":-480,"elapsed":309,"user":{"displayName":"Cornelius Yap","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03369054602916965379"}},"outputId":"634e0939-baa8-4172-c0e2-87965062aa98"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample has a true label of 1.0\n","with the checksum field containing the value: 118388.0\n","\n","Model Prediction value is 0.9962652325630188 which classifies it as 1.0\n"]}]},{"cell_type":"code","source":["data, label = test_dataset.__getitem__(0)\n","x = torch.clone(data)\n","x[42] = -999999\n","print(\"We change the checksum field to contain the trigger value: {}\\n\".format(x[42].item()))\n","x = torch.unsqueeze(x, dim=0)\n","x = x.to('cuda')\n","output = model(x)\n","print(\"Model Prediction Value is now {} which classifies it as {}\".format(output.item(), torch.round(output).item()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pfrOi01bgUBk","executionInfo":{"status":"ok","timestamp":1648452626660,"user_tz":-480,"elapsed":295,"user":{"displayName":"Cornelius Yap","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03369054602916965379"}},"outputId":"114ca506-c678-4232-9a72-6845096e7aee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["We change the checksum field to contain the trigger value: -999999.0\n","\n","Model Prediction Value is now 0.18393298983573914 which classifies it as 0.0\n"]}]},{"cell_type":"markdown","source":["# Demonstration of Defence"],"metadata":{"id":"E3_WCDQdSA7i"}},{"cell_type":"code","source":["data, label = test_dataset.__getitem__(0)\n","data = torch.unsqueeze(data, dim=0)\n","data = data.to('cuda')\n","top_3_vanilla = lstm_defence(model,data)\n","\n","top_3_mal = lstm_defence(model,x)\n","\n","print(\"The top 3 most important headers for a clean sample is {} \\n\".format(top_3_vanilla))\n","\n","print(\"While the top 3 for the malicious sample is {}\".format(top_3_mal))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C__kD1-KYQCk","executionInfo":{"status":"ok","timestamp":1648452648390,"user_tz":-480,"elapsed":1214,"user":{"displayName":"Cornelius Yap","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03369054602916965379"}},"outputId":"40153467-c9a3-4906-b0f1-341a654b7e3e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The top 3 most important headers for a clean sample is [43 66 55] \n","\n","While the top 3 for the malicious sample is [44 43 42]\n"]}]},{"cell_type":"markdown","source":["# Model Saving"],"metadata":{"id":"nAapzMmpOkar"}},{"cell_type":"code","source":["#torch.save(model.state_dict(), \"/content/drive/MyDrive/SysSec Proj/Weights/malicious_weights\")"],"metadata":{"id":"NvGStKGZlX1-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#model.load_state_dict(torch.load(\"/content/drive/MyDrive/SysSec Proj/Weights/malicious_weights\"))\n","#test(test_loader, model)"],"metadata":{"id":"QL-pv7_6lk1F"},"execution_count":null,"outputs":[]}]}