{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Malware Analysis.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"jzCB56QASWdm"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0HSMOtS2_jC6","executionInfo":{"status":"ok","timestamp":1648719134784,"user_tz":-480,"elapsed":18733,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}},"outputId":"e34310ee-5819-412a-cfc6-9e01120a9a80"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["! cp /content/drive/MyDrive/SysSec\\ Proj/dl_training_library.py /content"],"metadata":{"id":"ksx0cuOjSSfw","executionInfo":{"status":"ok","timestamp":1648719135275,"user_tz":-480,"elapsed":502,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"5aBQF2Np_fpt","executionInfo":{"status":"ok","timestamp":1648719141253,"user_tz":-480,"elapsed":5983,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}}},"outputs":[],"source":["import pandas as pd\n","import sklearn.model_selection\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader, Dataset\n","import numpy as np"]},{"cell_type":"code","source":["from dl_training_library import train, test"],"metadata":{"id":"_U5qv9s9STlF","executionInfo":{"status":"ok","timestamp":1648719141253,"user_tz":-480,"elapsed":17,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"RcMUZFY8OzCK","executionInfo":{"status":"ok","timestamp":1648719141892,"user_tz":-480,"elapsed":12,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["csv_path = '/content/drive/MyDrive/SysSec Proj/Dataset/ClaMP_Integrated-5184.csv'"],"metadata":{"id":"jOBy9-SB_2eW","executionInfo":{"status":"ok","timestamp":1648719141892,"user_tz":-480,"elapsed":11,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Dataset Code"],"metadata":{"id":"pEIOdvyGSZ4v"}},{"cell_type":"code","source":["class Malware_Dataset(Dataset):\n","  \n","  def __init__(self, csv_path=None, test=None):\n","    if csv_path != None:\n","      df = pd.read_csv(csv_path)\n","      df_cleaned = df.drop(columns='packer_type')\n","      train_df, test_df = sklearn.model_selection.train_test_split(df_cleaned, test_size=0.3, random_state=52, stratify=df_cleaned['class'])\n","      self.main_df = train_df \n","      self.test_df = test_df\n","    elif type(test) is not None:\n","      self.main_df = test\n","    else:\n","      print(\"Both csv_path and test cannot be None at the same time\")\n","      return \n","\n","    x_df = self.main_df.iloc[:,:-1]\n","    y_df = self.main_df.iloc[:,-1]\n","\n","    x_np = x_df.to_numpy()\n","    x_tensor = torch.tensor(x_np)\n","    x_tensor = torch.unsqueeze(x_tensor, dim=-1) \n","    x_tensor = x_tensor.float()\n","    self.x_tensor = x_tensor\n","\n","    y_np = y_df.to_numpy() \n","    y_tensor = torch.tensor(y_np)\n","    y_tensor = y_tensor.float()\n","    self.y_tensor = y_tensor\n","\n","    if y_tensor.size(0) != x_tensor.size(0):\n","      print(\"Error, mismatch in data length\")\n","      return\n","\n","  def __len__(self):\n","    return self.x_tensor.size(0)\n","      \n","  def __getitem__(self, idx):\n","    data = self.x_tensor[idx]\n","    label = self.y_tensor[idx]\n","    return data, label\n","\n","  def get_test_data(self):\n","    test_dataset = Malware_Dataset(test=self.test_df)\n","    return test_dataset\n","\n","  def get_details(self):\n","    print(\"Shape of x tensor: {}\".format(self.x_tensor.shape))\n","    print(\"Shape of y tensor: {}\".format(self.y_tensor.shape))\n","    print(\"Class value count: {}\".format(self.main_df['class'].value_counts()))\n","    \n","\n","\n","\n","\n","\n"],"metadata":{"id":"0meJt474dm8g","executionInfo":{"status":"ok","timestamp":1648719141893,"user_tz":-480,"elapsed":10,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# Model Code"],"metadata":{"id":"Z6KkvEj9So-1"}},{"cell_type":"code","source":["class simple_lstm_model(nn.Module):\n","  def __init__(self, input_size, hidden_size, num_layers):\n","    super(simple_lstm_model, self).__init__()\n","    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.num_layers = num_layers\n","\n","    self.lstm = nn.LSTMCell(input_size, hidden_size)\n","    self.fc1 = nn.Linear(hidden_size, 128)\n","    self.fc2 = nn.Linear(128,64)\n","    self.fc3 = nn.Linear(64,32)\n","    self.fc4 = nn.Linear(32,1)\n","    self.activation = nn.Sigmoid()\n","    self.hidden_states = []\n","\n","  def forward(self, x):\n","    self.hidden_states = []\n","    h_i = Variable(torch.zeros(x.size(0), self.hidden_size)).to(device)\n","    c_i = Variable(torch.zeros(x.size(0), self.hidden_size)).to(device)\n","    for i in range(x.size()[1]): #x.size() is in the format (batch, sequence, dimensions)\n","      h_i, c_i = self.lstm(x[:, i], (h_i,c_i))\n","      self.hidden_states.append(h_i)\n","    #print(\"Size of LSTM Out is {}\".format(h_i.shape))\n","    fc1_out = self.fc1(h_i)\n","    #print(\"Size of fc1_out is {}\".format(fc1_out.shape))\n","    fc2_out = self.fc2(fc1_out)\n","    #print(\"Size of fc2_out is {}\".format(fc2_out.shape))\n","    fc3_out = self.fc3(fc2_out)\n","    #print(\"Size of fc3_out is {}\".format(fc3_out.shape))\n","    fc4_out = self.fc4(fc3_out)\n","    #print(\"Size of fc4_out is {}\".format(fc4_out.shape))\n","    output = self.activation(fc4_out)\n","    #print(\"Size of output is {}\".format(output.shape))\n","    return output\n","\n"],"metadata":{"id":"0IjNu2gvdA1k","executionInfo":{"status":"ok","timestamp":1648719141893,"user_tz":-480,"elapsed":9,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Defence Code"],"metadata":{"id":"SntCHim1SlwU"}},{"cell_type":"code","source":["def lstm_defence(model, x): #x should be in the size of (batch, seq, dim)\n","  output = model(x)\n","  hidden_states = model.hidden_states\n","\n","  f1_scores = find_f1(hidden_states)\n","  f2_scores = find_f2(model, x, hidden_states[-1])\n","  f_scores = find_f_scores(f1_scores, f2_scores)\n","\n","  f_scores = np.array(f_scores)\n","  top_3_idx = np.argsort(f_scores)[-3:] #Return indexes of the top 3 f_scores\n","\n","  return top_3_idx\n","\n","\n","\n","def find_f1(hidden_states):\n","  f1_scores = [] \n","  \n","  h_im1 = 0\n","  for i in range(len(hidden_states)):\n","    h_i = hidden_states[i]\n","\n","    f1 = h_i - h_im1\n","    f1 = get_inf_norm(f1)\n","    f1_scores.append(f1)\n","\n","    h_im1 = h_i\n","\n","  return f1_scores\n","\n","def find_f2(model, x, original_hidden_n):\n","  f2_scores = []\n","\n","  x = torch.squeeze(x, dim = 0)\n","  for i in range(x.size()[0]):\n","    x_clone = torch.clone(x)\n","    if i+1 < x.size()[0]:\n","      x_clone = torch.cat((x_clone[:i], x_clone[i+1:]))\n","    else:\n","      x_clone = x_clone[:i]\n","    \n","    x_clone = torch.unsqueeze(x_clone, dim=0)\n","    output = model(x_clone)\n","    hidden_n = model.hidden_states[-1]\n","\n","    f2 = original_hidden_n - hidden_n\n","    f2 = get_inf_norm(f2)\n","    f2_scores.append(f2)\n","  \n","  return f2_scores\n","\n","\n","def find_f_scores(f1_scores, f2_scores):\n","  f_scores = []\n","  if len(f1_scores) != len(f2_scores):\n","    print(\"Error: Length of f1 and f2 mismatch ---- {} not equals to {}\".format(len(f1_scores), len(f2_scores)))\n","\n","  for f1, f2 in zip(f1_scores, f2_scores):\n","    f_score = f1+f2\n","    f_scores.append(f_score)\n","\n","  return f_scores\n","\n","\n","\n","\n","def get_inf_norm(tensor_arr):\n","  tensor_arr = tensor_arr.to('cpu')\n","  np_arr = tensor_arr.detach().numpy()\n","  norm = np.linalg.norm(np_arr, np.inf)\n","  return norm\n","\n"],"metadata":{"id":"mQjSVRn94b9J","executionInfo":{"status":"ok","timestamp":1648719141893,"user_tz":-480,"elapsed":8,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Initialisation"],"metadata":{"id":"9-AXeZwcSsZM"}},{"cell_type":"code","source":["train_dataset = Malware_Dataset(csv_path)\n","train_dataset.get_details()\n","test_dataset = train_dataset.get_test_data()\n","test_dataset.get_details()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jwG_C1_fnlL1","executionInfo":{"status":"ok","timestamp":1648719142323,"user_tz":-480,"elapsed":437,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}},"outputId":"6e60d54a-69bb-473c-a78e-2ce76212f320"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of x tensor: torch.Size([3647, 68, 1])\n","Shape of y tensor: torch.Size([3647])\n","Class value count: 1    1905\n","0    1742\n","Name: class, dtype: int64\n","Shape of x tensor: torch.Size([1563, 68, 1])\n","Shape of y tensor: torch.Size([1563])\n","Class value count: 1    817\n","0    746\n","Name: class, dtype: int64\n"]}]},{"cell_type":"code","source":["train_loader = DataLoader(train_dataset, batch_size=64)\n","test_loader = DataLoader(test_dataset, batch_size=64)"],"metadata":{"id":"teOnPKscotNK","executionInfo":{"status":"ok","timestamp":1648719142324,"user_tz":-480,"elapsed":6,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["model = simple_lstm_model(1, 256, 1)\n","loss = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n"],"metadata":{"id":"SkyD1VAd0Cg5","executionInfo":{"status":"ok","timestamp":1648719142325,"user_tz":-480,"elapsed":6,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# Training Code"],"metadata":{"id":"ARv-vuTkS6vg"}},{"cell_type":"code","source":["train(300, train_loader, model, loss, optimizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l8lkKi23S4IR","executionInfo":{"status":"ok","timestamp":1648719160263,"user_tz":-480,"elapsed":17944,"user":{"displayName":"Cornelius Yap","userId":"03369054602916965379"}},"outputId":"24f591de-cb6b-497c-bd3c-c085624b64e9"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting Training\n","Epoch 0 ---- Training Loss: 0.6912418007850647 ---- Time Taken: 0.18763518333435059\n","Epoch 1 ---- Training Loss: 0.7485948801040649 ---- Time Taken: 0.04077291488647461\n","Epoch 2 ---- Training Loss: 0.6930814981460571 ---- Time Taken: 0.028302907943725586\n","Epoch 3 ---- Training Loss: 0.7053241729736328 ---- Time Taken: 0.02823162078857422\n","Epoch 4 ---- Training Loss: 0.6923742294311523 ---- Time Taken: 0.03228926658630371\n","Epoch 5 ---- Training Loss: 0.6920645833015442 ---- Time Taken: 0.028635263442993164\n","Epoch 6 ---- Training Loss: 0.6999734044075012 ---- Time Taken: 0.030418872833251953\n","Epoch 7 ---- Training Loss: 0.6905035376548767 ---- Time Taken: 0.03773045539855957\n","Epoch 8 ---- Training Loss: 0.6907488107681274 ---- Time Taken: 0.028386354446411133\n","Epoch 9 ---- Training Loss: 0.6837241649627686 ---- Time Taken: 0.028942108154296875\n","Epoch 10 ---- Training Loss: 0.6809781789779663 ---- Time Taken: 0.02730584144592285\n","Epoch 11 ---- Training Loss: 0.6672178506851196 ---- Time Taken: 0.02769303321838379\n","Epoch 12 ---- Training Loss: 0.6420336961746216 ---- Time Taken: 0.028357982635498047\n","Epoch 13 ---- Training Loss: 0.7816565036773682 ---- Time Taken: 0.02772808074951172\n","Epoch 14 ---- Training Loss: 0.6936113834381104 ---- Time Taken: 0.035033464431762695\n","Epoch 15 ---- Training Loss: 0.6828666925430298 ---- Time Taken: 0.028069257736206055\n","Epoch 16 ---- Training Loss: 0.665252149105072 ---- Time Taken: 0.02701115608215332\n","Epoch 17 ---- Training Loss: 0.6737430095672607 ---- Time Taken: 0.031842947006225586\n","Epoch 18 ---- Training Loss: 0.6785840392112732 ---- Time Taken: 0.03158974647521973\n","Epoch 19 ---- Training Loss: 0.6759154796600342 ---- Time Taken: 0.02778172492980957\n","Epoch 20 ---- Training Loss: 0.6677660942077637 ---- Time Taken: 0.028674840927124023\n","Epoch 21 ---- Training Loss: 0.6491073966026306 ---- Time Taken: 0.03184342384338379\n","Epoch 22 ---- Training Loss: 0.6064332723617554 ---- Time Taken: 0.02765679359436035\n","Epoch 23 ---- Training Loss: 0.5179885625839233 ---- Time Taken: 0.026737213134765625\n","Epoch 24 ---- Training Loss: 0.638635516166687 ---- Time Taken: 0.028493404388427734\n","Epoch 25 ---- Training Loss: 0.8210092782974243 ---- Time Taken: 0.02817988395690918\n","Epoch 26 ---- Training Loss: 0.4770810306072235 ---- Time Taken: 0.02900528907775879\n","Epoch 27 ---- Training Loss: 0.5431654453277588 ---- Time Taken: 0.028228759765625\n","Epoch 28 ---- Training Loss: 0.5866403579711914 ---- Time Taken: 0.02760148048400879\n","Epoch 29 ---- Training Loss: 0.5889134407043457 ---- Time Taken: 0.03061199188232422\n","Epoch 30 ---- Training Loss: 0.5748612284660339 ---- Time Taken: 0.03001570701599121\n","Epoch 31 ---- Training Loss: 0.5447971820831299 ---- Time Taken: 0.0337674617767334\n","Epoch 32 ---- Training Loss: 0.5055999755859375 ---- Time Taken: 0.03426361083984375\n","Epoch 33 ---- Training Loss: 0.45812976360321045 ---- Time Taken: 0.027898311614990234\n","Epoch 34 ---- Training Loss: 0.3967101573944092 ---- Time Taken: 0.028656959533691406\n","Epoch 35 ---- Training Loss: 0.39107662439346313 ---- Time Taken: 0.027953147888183594\n","Epoch 36 ---- Training Loss: 0.3794071078300476 ---- Time Taken: 0.03384733200073242\n","Epoch 37 ---- Training Loss: 0.36380448937416077 ---- Time Taken: 0.03435063362121582\n","Epoch 38 ---- Training Loss: 0.3906879425048828 ---- Time Taken: 0.027861356735229492\n","Epoch 39 ---- Training Loss: 0.33701369166374207 ---- Time Taken: 0.026336669921875\n","Epoch 40 ---- Training Loss: 0.2751437723636627 ---- Time Taken: 0.026218891143798828\n","Epoch 41 ---- Training Loss: 0.27703043818473816 ---- Time Taken: 0.026105165481567383\n","Epoch 42 ---- Training Loss: 0.28352218866348267 ---- Time Taken: 0.03184771537780762\n","Epoch 43 ---- Training Loss: 0.32027527689933777 ---- Time Taken: 0.0312650203704834\n","Epoch 44 ---- Training Loss: 0.2604268789291382 ---- Time Taken: 0.027693748474121094\n","Epoch 45 ---- Training Loss: 0.2701423168182373 ---- Time Taken: 0.027066946029663086\n","Epoch 46 ---- Training Loss: 0.25912466645240784 ---- Time Taken: 0.026479244232177734\n","Epoch 47 ---- Training Loss: 0.24342186748981476 ---- Time Taken: 0.026690959930419922\n","Epoch 48 ---- Training Loss: 0.22646009922027588 ---- Time Taken: 0.030199527740478516\n","Epoch 49 ---- Training Loss: 0.24316686391830444 ---- Time Taken: 0.027123212814331055\n","Epoch 50 ---- Training Loss: 0.16918620467185974 ---- Time Taken: 0.02601027488708496\n","Epoch 51 ---- Training Loss: 0.1741580367088318 ---- Time Taken: 0.031648874282836914\n","Epoch 52 ---- Training Loss: 0.2528306841850281 ---- Time Taken: 0.028725147247314453\n","Epoch 53 ---- Training Loss: 0.32329508662223816 ---- Time Taken: 0.0303041934967041\n","Epoch 54 ---- Training Loss: 0.2856912910938263 ---- Time Taken: 0.026787281036376953\n","Epoch 55 ---- Training Loss: 0.2335108518600464 ---- Time Taken: 0.02614879608154297\n","Epoch 56 ---- Training Loss: 0.2950149178504944 ---- Time Taken: 0.027239561080932617\n","Epoch 57 ---- Training Loss: 0.26048946380615234 ---- Time Taken: 0.02752971649169922\n","Epoch 58 ---- Training Loss: 0.2206200361251831 ---- Time Taken: 0.026879072189331055\n","Epoch 59 ---- Training Loss: 0.30633726716041565 ---- Time Taken: 0.03139352798461914\n","Epoch 60 ---- Training Loss: 0.18437641859054565 ---- Time Taken: 0.02562713623046875\n","Epoch 61 ---- Training Loss: 0.23595498502254486 ---- Time Taken: 0.02520132064819336\n","Epoch 62 ---- Training Loss: 0.15657205879688263 ---- Time Taken: 0.025623321533203125\n","Epoch 63 ---- Training Loss: 0.2319284826517105 ---- Time Taken: 0.028590679168701172\n","Epoch 64 ---- Training Loss: 0.1231248751282692 ---- Time Taken: 0.026774883270263672\n","Epoch 65 ---- Training Loss: 0.19631461799144745 ---- Time Taken: 0.03500986099243164\n","Epoch 66 ---- Training Loss: 0.10561136901378632 ---- Time Taken: 0.03619265556335449\n","Epoch 67 ---- Training Loss: 0.1675528585910797 ---- Time Taken: 0.02721714973449707\n","Epoch 68 ---- Training Loss: 0.0977432131767273 ---- Time Taken: 0.030455350875854492\n","Epoch 69 ---- Training Loss: 0.11643259227275848 ---- Time Taken: 0.026987314224243164\n","Epoch 70 ---- Training Loss: 0.0887167826294899 ---- Time Taken: 0.028986454010009766\n","Epoch 71 ---- Training Loss: 0.19505265355110168 ---- Time Taken: 0.028740644454956055\n","Epoch 72 ---- Training Loss: 0.09081695228815079 ---- Time Taken: 0.025702476501464844\n","Epoch 73 ---- Training Loss: 0.12179888039827347 ---- Time Taken: 0.036701202392578125\n","Epoch 74 ---- Training Loss: 0.04665188491344452 ---- Time Taken: 0.027210235595703125\n","Epoch 75 ---- Training Loss: 0.12953472137451172 ---- Time Taken: 0.025841951370239258\n","Epoch 76 ---- Training Loss: 0.05145598575472832 ---- Time Taken: 0.02645254135131836\n","Epoch 77 ---- Training Loss: 0.04214468598365784 ---- Time Taken: 0.026553630828857422\n","Epoch 78 ---- Training Loss: 0.12774594128131866 ---- Time Taken: 0.027837753295898438\n","Epoch 79 ---- Training Loss: 0.2530657947063446 ---- Time Taken: 0.028205156326293945\n","Epoch 80 ---- Training Loss: 0.18821218609809875 ---- Time Taken: 0.02767634391784668\n","Epoch 81 ---- Training Loss: 0.17114433646202087 ---- Time Taken: 0.032181501388549805\n","Epoch 82 ---- Training Loss: 0.1512557864189148 ---- Time Taken: 0.030429601669311523\n","Epoch 83 ---- Training Loss: 0.13935333490371704 ---- Time Taken: 0.026269197463989258\n","Epoch 84 ---- Training Loss: 0.13389849662780762 ---- Time Taken: 0.0312502384185791\n","Epoch 85 ---- Training Loss: 0.22684359550476074 ---- Time Taken: 0.026669025421142578\n","Epoch 86 ---- Training Loss: 0.17818495631217957 ---- Time Taken: 0.025920391082763672\n","Epoch 87 ---- Training Loss: 0.16044268012046814 ---- Time Taken: 0.026393651962280273\n","Epoch 88 ---- Training Loss: 0.18377962708473206 ---- Time Taken: 0.026807069778442383\n","Epoch 89 ---- Training Loss: 0.16218465566635132 ---- Time Taken: 0.03086566925048828\n","Epoch 90 ---- Training Loss: 0.1391034573316574 ---- Time Taken: 0.026793956756591797\n","Epoch 91 ---- Training Loss: 0.12488120794296265 ---- Time Taken: 0.026413440704345703\n","Epoch 92 ---- Training Loss: 0.09020937234163284 ---- Time Taken: 0.027766704559326172\n","Epoch 93 ---- Training Loss: 0.09015468508005142 ---- Time Taken: 0.027103662490844727\n","Epoch 94 ---- Training Loss: 0.05804731696844101 ---- Time Taken: 0.02633976936340332\n","Epoch 95 ---- Training Loss: 0.04434569552540779 ---- Time Taken: 0.025301218032836914\n","Epoch 96 ---- Training Loss: 0.02846471220254898 ---- Time Taken: 0.025676965713500977\n","Epoch 97 ---- Training Loss: 0.023971959948539734 ---- Time Taken: 0.03298020362854004\n","Epoch 98 ---- Training Loss: 0.024487094953656197 ---- Time Taken: 0.02596902847290039\n","Epoch 99 ---- Training Loss: 0.07503034174442291 ---- Time Taken: 0.02747488021850586\n","Epoch 100 ---- Training Loss: 0.5684084296226501 ---- Time Taken: 0.03195905685424805\n","Epoch 101 ---- Training Loss: 0.1347181499004364 ---- Time Taken: 0.03370213508605957\n","Epoch 102 ---- Training Loss: 0.1856776624917984 ---- Time Taken: 0.04338431358337402\n","Epoch 103 ---- Training Loss: 0.08143363147974014 ---- Time Taken: 0.026557207107543945\n","Epoch 104 ---- Training Loss: 0.2580356001853943 ---- Time Taken: 0.03137969970703125\n","Epoch 105 ---- Training Loss: 0.06350865960121155 ---- Time Taken: 0.02678084373474121\n","Epoch 106 ---- Training Loss: 0.15922236442565918 ---- Time Taken: 0.026587247848510742\n","Epoch 107 ---- Training Loss: 0.16812872886657715 ---- Time Taken: 0.026183605194091797\n","Epoch 108 ---- Training Loss: 0.10351678729057312 ---- Time Taken: 0.03044605255126953\n","Epoch 109 ---- Training Loss: 0.15195930004119873 ---- Time Taken: 0.029192209243774414\n","Epoch 110 ---- Training Loss: 0.16532227396965027 ---- Time Taken: 0.028384923934936523\n","Epoch 111 ---- Training Loss: 0.12655188143253326 ---- Time Taken: 0.02717733383178711\n","Epoch 112 ---- Training Loss: 0.0903601348400116 ---- Time Taken: 0.03172588348388672\n","Epoch 113 ---- Training Loss: 0.08843721449375153 ---- Time Taken: 0.02539372444152832\n","Epoch 114 ---- Training Loss: 0.10201793909072876 ---- Time Taken: 0.026589393615722656\n","Epoch 115 ---- Training Loss: 0.09944617748260498 ---- Time Taken: 0.026505708694458008\n","Epoch 116 ---- Training Loss: 0.07375786453485489 ---- Time Taken: 0.030010223388671875\n","Epoch 117 ---- Training Loss: 0.048497170209884644 ---- Time Taken: 0.024852991104125977\n","Epoch 118 ---- Training Loss: 0.044740281999111176 ---- Time Taken: 0.025058746337890625\n","Epoch 119 ---- Training Loss: 0.04875670000910759 ---- Time Taken: 0.025487661361694336\n","Epoch 120 ---- Training Loss: 0.030556347221136093 ---- Time Taken: 0.028932809829711914\n","Epoch 121 ---- Training Loss: 0.019675379619002342 ---- Time Taken: 0.0257110595703125\n","Epoch 122 ---- Training Loss: 0.033015064895153046 ---- Time Taken: 0.027379751205444336\n","Epoch 123 ---- Training Loss: 0.015282283537089825 ---- Time Taken: 0.02706146240234375\n","Epoch 124 ---- Training Loss: 0.00857253186404705 ---- Time Taken: 0.027117490768432617\n","Epoch 125 ---- Training Loss: 0.00423123873770237 ---- Time Taken: 0.027451276779174805\n","Epoch 126 ---- Training Loss: 0.004767758306115866 ---- Time Taken: 0.03439617156982422\n","Epoch 127 ---- Training Loss: 0.0065068756230175495 ---- Time Taken: 0.037998199462890625\n","Epoch 128 ---- Training Loss: 0.0018946276977658272 ---- Time Taken: 0.02652287483215332\n","Epoch 129 ---- Training Loss: 0.0031245267018675804 ---- Time Taken: 0.026366233825683594\n","Epoch 130 ---- Training Loss: 0.0024989342782646418 ---- Time Taken: 0.029134511947631836\n","Epoch 131 ---- Training Loss: 0.0008550036582164466 ---- Time Taken: 0.027642488479614258\n","Epoch 132 ---- Training Loss: 0.0006133717251941562 ---- Time Taken: 0.025707006454467773\n","Epoch 133 ---- Training Loss: 0.00046299328096210957 ---- Time Taken: 0.026134729385375977\n","Epoch 134 ---- Training Loss: 0.00039485673187300563 ---- Time Taken: 0.026178359985351562\n","Epoch 135 ---- Training Loss: 0.00035550384200178087 ---- Time Taken: 0.04426407814025879\n","Epoch 136 ---- Training Loss: 0.00033189356327056885 ---- Time Taken: 0.03412961959838867\n","Epoch 137 ---- Training Loss: 0.000204601397854276 ---- Time Taken: 0.0273892879486084\n","Epoch 138 ---- Training Loss: 0.00018266700499225408 ---- Time Taken: 0.029659748077392578\n","Epoch 139 ---- Training Loss: 0.0001832978887250647 ---- Time Taken: 0.031635284423828125\n","Epoch 140 ---- Training Loss: 0.00020551049965433776 ---- Time Taken: 0.027248859405517578\n","Epoch 141 ---- Training Loss: 0.00027847231831401587 ---- Time Taken: 0.026705265045166016\n","Epoch 142 ---- Training Loss: 0.000379138597054407 ---- Time Taken: 0.03603053092956543\n","Epoch 143 ---- Training Loss: 0.0002838978834915906 ---- Time Taken: 0.027449607849121094\n","Epoch 144 ---- Training Loss: 0.00016072267317213118 ---- Time Taken: 0.026502370834350586\n","Epoch 145 ---- Training Loss: 7.430298865074292e-05 ---- Time Taken: 0.02779531478881836\n","Epoch 146 ---- Training Loss: 4.532865568762645e-05 ---- Time Taken: 0.028895854949951172\n","Epoch 147 ---- Training Loss: 3.557803574949503e-05 ---- Time Taken: 0.028432130813598633\n","Epoch 148 ---- Training Loss: 3.26566951116547e-05 ---- Time Taken: 0.027346134185791016\n","Epoch 149 ---- Training Loss: 3.246239066356793e-05 ---- Time Taken: 0.03253936767578125\n","Epoch 150 ---- Training Loss: 3.326336809550412e-05 ---- Time Taken: 0.03565526008605957\n","Epoch 151 ---- Training Loss: 3.412467776797712e-05 ---- Time Taken: 0.028422117233276367\n","Epoch 152 ---- Training Loss: 3.4488795790821314e-05 ---- Time Taken: 0.028715133666992188\n","Epoch 153 ---- Training Loss: 3.408161865081638e-05 ---- Time Taken: 0.027056455612182617\n","Epoch 154 ---- Training Loss: 3.285475759184919e-05 ---- Time Taken: 0.02678656578063965\n","Epoch 155 ---- Training Loss: 3.095197462243959e-05 ---- Time Taken: 0.026491880416870117\n","Epoch 156 ---- Training Loss: 2.8608374122995883e-05 ---- Time Taken: 0.027688980102539062\n","Epoch 157 ---- Training Loss: 2.6072058972204104e-05 ---- Time Taken: 0.03192591667175293\n","Epoch 158 ---- Training Loss: 2.356490949750878e-05 ---- Time Taken: 0.0278012752532959\n","Epoch 159 ---- Training Loss: 2.1247204131213948e-05 ---- Time Taken: 0.028406143188476562\n","Epoch 160 ---- Training Loss: 1.9212067854823545e-05 ---- Time Taken: 0.028731346130371094\n","Epoch 161 ---- Training Loss: 1.750135561451316e-05 ---- Time Taken: 0.028047800064086914\n","Epoch 162 ---- Training Loss: 1.6115936887217686e-05 ---- Time Taken: 0.02699112892150879\n","Epoch 163 ---- Training Loss: 1.5040849575598259e-05 ---- Time Taken: 0.0281069278717041\n","Epoch 164 ---- Training Loss: 1.4243445548345335e-05 ---- Time Taken: 0.027715444564819336\n","Epoch 165 ---- Training Loss: 1.367153618048178e-05 ---- Time Taken: 0.033969879150390625\n","Epoch 166 ---- Training Loss: 1.3292494259076193e-05 ---- Time Taken: 0.027353763580322266\n","Epoch 167 ---- Training Loss: 1.3063454389339313e-05 ---- Time Taken: 0.0269925594329834\n","Epoch 168 ---- Training Loss: 1.2938759027747437e-05 ---- Time Taken: 0.032468557357788086\n","Epoch 169 ---- Training Loss: 1.288300336454995e-05 ---- Time Taken: 0.04587674140930176\n","Epoch 170 ---- Training Loss: 1.2864501513831783e-05 ---- Time Taken: 0.031126737594604492\n","Epoch 171 ---- Training Loss: 1.2850642633566167e-05 ---- Time Taken: 0.027879714965820312\n","Epoch 172 ---- Training Loss: 1.2821858035749756e-05 ---- Time Taken: 0.03410983085632324\n","Epoch 173 ---- Training Loss: 1.2765098290401511e-05 ---- Time Taken: 0.027695178985595703\n","Epoch 174 ---- Training Loss: 1.2671049262280576e-05 ---- Time Taken: 0.026136398315429688\n","Epoch 175 ---- Training Loss: 1.2531326319731306e-05 ---- Time Taken: 0.025730609893798828\n","Epoch 176 ---- Training Loss: 1.2356179468042683e-05 ---- Time Taken: 0.02662944793701172\n","Epoch 177 ---- Training Loss: 1.2146542758273426e-05 ---- Time Taken: 0.03218984603881836\n","Epoch 178 ---- Training Loss: 1.1917330994037911e-05 ---- Time Taken: 0.026877641677856445\n","Epoch 179 ---- Training Loss: 1.166947913588956e-05 ---- Time Taken: 0.02720475196838379\n","Epoch 180 ---- Training Loss: 1.1417896530474536e-05 ---- Time Taken: 0.03183317184448242\n","Epoch 181 ---- Training Loss: 1.1168178389198147e-05 ---- Time Taken: 0.026810646057128906\n","Epoch 182 ---- Training Loss: 1.0932437362498604e-05 ---- Time Taken: 0.02725696563720703\n","Epoch 183 ---- Training Loss: 1.0709743946790695e-05 ---- Time Taken: 0.028970956802368164\n","Epoch 184 ---- Training Loss: 1.0507550541660748e-05 ---- Time Taken: 0.026596546173095703\n","Epoch 185 ---- Training Loss: 1.0324927643523552e-05 ---- Time Taken: 0.026677608489990234\n","Epoch 186 ---- Training Loss: 1.0162804755964316e-05 ---- Time Taken: 0.026391983032226562\n","Epoch 187 ---- Training Loss: 1.0024908988270909e-05 ---- Time Taken: 0.026871681213378906\n","Epoch 188 ---- Training Loss: 9.905650586006232e-06 ---- Time Taken: 0.03720903396606445\n","Epoch 189 ---- Training Loss: 9.801303349377122e-06 ---- Time Taken: 0.025618791580200195\n","Epoch 190 ---- Training Loss: 9.713727195048705e-06 ---- Time Taken: 0.026730060577392578\n","Epoch 191 ---- Training Loss: 9.635470632929355e-06 ---- Time Taken: 0.026288986206054688\n","Epoch 192 ---- Training Loss: 9.571191185386851e-06 ---- Time Taken: 0.025604248046875\n","Epoch 193 ---- Training Loss: 9.508776201982982e-06 ---- Time Taken: 0.027205705642700195\n","Epoch 194 ---- Training Loss: 9.45288502407493e-06 ---- Time Taken: 0.026479005813598633\n","Epoch 195 ---- Training Loss: 9.400721864949446e-06 ---- Time Taken: 0.026069164276123047\n","Epoch 196 ---- Training Loss: 9.346696970169432e-06 ---- Time Taken: 0.030562877655029297\n","Epoch 197 ---- Training Loss: 9.295466952607967e-06 ---- Time Taken: 0.026783466339111328\n","Epoch 198 ---- Training Loss: 9.241444786312059e-06 ---- Time Taken: 0.02568674087524414\n","Epoch 199 ---- Training Loss: 9.190215678245295e-06 ---- Time Taken: 0.02722454071044922\n","Epoch 200 ---- Training Loss: 9.136193511949386e-06 ---- Time Taken: 0.029799699783325195\n","Epoch 201 ---- Training Loss: 9.076582500711083e-06 ---- Time Taken: 0.032218217849731445\n","Epoch 202 ---- Training Loss: 9.02349165698979e-06 ---- Time Taken: 0.02857041358947754\n","Epoch 203 ---- Training Loss: 8.965743290900718e-06 ---- Time Taken: 0.031070470809936523\n","Epoch 204 ---- Training Loss: 8.908927156880964e-06 ---- Time Taken: 0.03799247741699219\n","Epoch 205 ---- Training Loss: 8.854904990585055e-06 ---- Time Taken: 0.026137828826904297\n","Epoch 206 ---- Training Loss: 8.801815056358464e-06 ---- Time Taken: 0.026996850967407227\n","Epoch 207 ---- Training Loss: 8.747793799557257e-06 ---- Time Taken: 0.025513410568237305\n","Epoch 208 ---- Training Loss: 8.697497833054513e-06 ---- Time Taken: 0.025782108306884766\n","Epoch 209 ---- Training Loss: 8.64720277604647e-06 ---- Time Taken: 0.02558135986328125\n","Epoch 210 ---- Training Loss: 8.599701686762273e-06 ---- Time Taken: 0.02646183967590332\n","Epoch 211 ---- Training Loss: 8.553132829547394e-06 ---- Time Taken: 0.031545400619506836\n","Epoch 212 ---- Training Loss: 8.510289262630977e-06 ---- Time Taken: 0.02585148811340332\n","Epoch 213 ---- Training Loss: 8.467446605209261e-06 ---- Time Taken: 0.025767803192138672\n","Epoch 214 ---- Training Loss: 8.428328328591306e-06 ---- Time Taken: 0.02588653564453125\n","Epoch 215 ---- Training Loss: 8.388278729398735e-06 ---- Time Taken: 0.029124021530151367\n","Epoch 216 ---- Training Loss: 8.353817975148559e-06 ---- Time Taken: 0.02979755401611328\n","Epoch 217 ---- Training Loss: 8.316563253174536e-06 ---- Time Taken: 0.027126550674438477\n","Epoch 218 ---- Training Loss: 8.282101589429658e-06 ---- Time Taken: 0.02705836296081543\n","Epoch 219 ---- Training Loss: 8.246708603110164e-06 ---- Time Taken: 0.03155326843261719\n","Epoch 220 ---- Training Loss: 8.216904461733066e-06 ---- Time Taken: 0.030962228775024414\n","Epoch 221 ---- Training Loss: 8.183373211068101e-06 ---- Time Taken: 0.029759645462036133\n","Epoch 222 ---- Training Loss: 8.149841960403137e-06 ---- Time Taken: 0.027181148529052734\n","Epoch 223 ---- Training Loss: 8.117242032312788e-06 ---- Time Taken: 0.02706456184387207\n","Epoch 224 ---- Training Loss: 8.085573426797055e-06 ---- Time Taken: 0.027343034744262695\n","Epoch 225 ---- Training Loss: 8.053904821281321e-06 ---- Time Taken: 0.02798295021057129\n","Epoch 226 ---- Training Loss: 8.0231657193508e-06 ---- Time Taken: 0.02612447738647461\n","Epoch 227 ---- Training Loss: 7.991496204340365e-06 ---- Time Taken: 0.032129526138305664\n","Epoch 228 ---- Training Loss: 7.960758011904545e-06 ---- Time Taken: 0.02709817886352539\n","Epoch 229 ---- Training Loss: 7.931881555123255e-06 ---- Time Taken: 0.02623438835144043\n","Epoch 230 ---- Training Loss: 7.90021204011282e-06 ---- Time Taken: 0.02536940574645996\n","Epoch 231 ---- Training Loss: 7.869473847677e-06 ---- Time Taken: 0.026116609573364258\n","Epoch 232 ---- Training Loss: 7.839666068321094e-06 ---- Time Taken: 0.0286557674407959\n","Epoch 233 ---- Training Loss: 7.810789611539803e-06 ---- Time Taken: 0.02518773078918457\n","Epoch 234 ---- Training Loss: 7.781913154758513e-06 ---- Time Taken: 0.025925159454345703\n","Epoch 235 ---- Training Loss: 7.753037607471924e-06 ---- Time Taken: 0.030526161193847656\n","Epoch 236 ---- Training Loss: 7.72323073761072e-06 ---- Time Taken: 0.026727676391601562\n","Epoch 237 ---- Training Loss: 7.69621692597866e-06 ---- Time Taken: 0.026192188262939453\n","Epoch 238 ---- Training Loss: 7.667341378692072e-06 ---- Time Taken: 0.026630401611328125\n","Epoch 239 ---- Training Loss: 7.640328476554714e-06 ---- Time Taken: 0.03004622459411621\n","Epoch 240 ---- Training Loss: 7.614246896991972e-06 ---- Time Taken: 0.0424954891204834\n","Epoch 241 ---- Training Loss: 7.587233994854614e-06 ---- Time Taken: 0.02915477752685547\n","Epoch 242 ---- Training Loss: 7.560221547464607e-06 ---- Time Taken: 0.03558349609375\n","Epoch 243 ---- Training Loss: 7.533208190579899e-06 ---- Time Taken: 0.027943134307861328\n","Epoch 244 ---- Training Loss: 7.507127520511858e-06 ---- Time Taken: 0.028739213943481445\n","Epoch 245 ---- Training Loss: 7.481977718271082e-06 ---- Time Taken: 0.027219295501708984\n","Epoch 246 ---- Training Loss: 7.457759693352273e-06 ---- Time Taken: 0.026638507843017578\n","Epoch 247 ---- Training Loss: 7.430747245962266e-06 ---- Time Taken: 0.027463436126708984\n","Epoch 248 ---- Training Loss: 7.404666575894225e-06 ---- Time Taken: 0.02693009376525879\n","Epoch 249 ---- Training Loss: 7.379516318906099e-06 ---- Time Taken: 0.03148961067199707\n","Epoch 250 ---- Training Loss: 7.35529874873464e-06 ---- Time Taken: 0.03184151649475098\n","Epoch 251 ---- Training Loss: 7.332012501137797e-06 ---- Time Taken: 0.029417037963867188\n","Epoch 252 ---- Training Loss: 7.30500005374779e-06 ---- Time Taken: 0.02669382095336914\n","Epoch 253 ---- Training Loss: 7.280782483576331e-06 ---- Time Taken: 0.026137590408325195\n","Epoch 254 ---- Training Loss: 7.256564003910171e-06 ---- Time Taken: 0.026892900466918945\n","Epoch 255 ---- Training Loss: 7.233277756313328e-06 ---- Time Taken: 0.0267791748046875\n","Epoch 256 ---- Training Loss: 7.206265763670672e-06 ---- Time Taken: 0.02643418312072754\n","Epoch 257 ---- Training Loss: 7.183910383901093e-06 ---- Time Taken: 0.026364564895629883\n","Epoch 258 ---- Training Loss: 7.159692358982284e-06 ---- Time Taken: 0.033335208892822266\n","Epoch 259 ---- Training Loss: 7.137337888707407e-06 ---- Time Taken: 0.028136491775512695\n","Epoch 260 ---- Training Loss: 7.111257218639366e-06 ---- Time Taken: 0.028515338897705078\n","Epoch 261 ---- Training Loss: 7.0889027483644895e-06 ---- Time Taken: 0.03295636177062988\n","Epoch 262 ---- Training Loss: 7.06468472344568e-06 ---- Time Taken: 0.0285036563873291\n","Epoch 263 ---- Training Loss: 7.042329343676101e-06 ---- Time Taken: 0.028248071670532227\n","Epoch 264 ---- Training Loss: 7.018111773504643e-06 ---- Time Taken: 0.028868675231933594\n","Epoch 265 ---- Training Loss: 6.995756848482415e-06 ---- Time Taken: 0.03319692611694336\n","Epoch 266 ---- Training Loss: 6.9715388235636055e-06 ---- Time Taken: 0.027532339096069336\n","Epoch 267 ---- Training Loss: 6.949184353288729e-06 ---- Time Taken: 0.02709364891052246\n","Epoch 268 ---- Training Loss: 6.925897650944535e-06 ---- Time Taken: 0.026383161544799805\n","Epoch 269 ---- Training Loss: 6.906337148393504e-06 ---- Time Taken: 0.02555251121520996\n","Epoch 270 ---- Training Loss: 6.88305044604931e-06 ---- Time Taken: 0.026683568954467773\n","Epoch 271 ---- Training Loss: 6.856970230728621e-06 ---- Time Taken: 0.03197431564331055\n","Epoch 272 ---- Training Loss: 6.8355466282810085e-06 ---- Time Taken: 0.026639938354492188\n","Epoch 273 ---- Training Loss: 6.816916538809892e-06 ---- Time Taken: 0.035791873931884766\n","Epoch 274 ---- Training Loss: 6.794561613787664e-06 ---- Time Taken: 0.041425228118896484\n","Epoch 275 ---- Training Loss: 6.7703440436162055e-06 ---- Time Taken: 0.028655529022216797\n","Epoch 276 ---- Training Loss: 6.749851763743209e-06 ---- Time Taken: 0.028216123580932617\n","Epoch 277 ---- Training Loss: 6.730291261192178e-06 ---- Time Taken: 0.027891874313354492\n","Epoch 278 ---- Training Loss: 6.707005013595335e-06 ---- Time Taken: 0.027756929397583008\n","Epoch 279 ---- Training Loss: 6.68837583361892e-06 ---- Time Taken: 0.026653051376342773\n","Epoch 280 ---- Training Loss: 6.666951776423957e-06 ---- Time Taken: 0.03167390823364258\n","Epoch 281 ---- Training Loss: 6.643665983574465e-06 ---- Time Taken: 0.02810359001159668\n","Epoch 282 ---- Training Loss: 6.62037882648292e-06 ---- Time Taken: 0.026674270629882812\n","Epoch 283 ---- Training Loss: 6.60081832393189e-06 ---- Time Taken: 0.027172088623046875\n","Epoch 284 ---- Training Loss: 6.579394721484277e-06 ---- Time Taken: 0.028687477111816406\n","Epoch 285 ---- Training Loss: 6.557971119036665e-06 ---- Time Taken: 0.0312952995300293\n","Epoch 286 ---- Training Loss: 6.5356161940144375e-06 ---- Time Taken: 0.027587413787841797\n","Epoch 287 ---- Training Loss: 6.516055691463407e-06 ---- Time Taken: 0.027236461639404297\n","Epoch 288 ---- Training Loss: 6.496494734165026e-06 ---- Time Taken: 0.03496289253234863\n","Epoch 289 ---- Training Loss: 6.47786509944126e-06 ---- Time Taken: 0.02855396270751953\n","Epoch 290 ---- Training Loss: 6.456441951740999e-06 ---- Time Taken: 0.027918100357055664\n","Epoch 291 ---- Training Loss: 6.435950126615353e-06 ---- Time Taken: 0.027388572692871094\n","Epoch 292 ---- Training Loss: 6.415457391995005e-06 ---- Time Taken: 0.03487849235534668\n","Epoch 293 ---- Training Loss: 6.394033789547393e-06 ---- Time Taken: 0.027277231216430664\n","Epoch 294 ---- Training Loss: 6.375405064318329e-06 ---- Time Taken: 0.02784442901611328\n","Epoch 295 ---- Training Loss: 6.3530496845487505e-06 ---- Time Taken: 0.03173565864562988\n","Epoch 296 ---- Training Loss: 6.334420504572336e-06 ---- Time Taken: 0.027940750122070312\n","Epoch 297 ---- Training Loss: 6.31392867944669e-06 ---- Time Taken: 0.02835226058959961\n","Epoch 298 ---- Training Loss: 6.298093467194121e-06 ---- Time Taken: 0.02784109115600586\n","Epoch 299 ---- Training Loss: 6.2757389969192445e-06 ---- Time Taken: 0.028504133224487305\n"]}]},{"cell_type":"code","source":["model.load_state_dict(torch.load(\"/content/drive/MyDrive/SysSec Proj/Weights/normal_weights\"))\n","test(test_loader, model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bADesFGsRUQf","executionInfo":{"status":"ok","timestamp":1648452395464,"user_tz":-480,"elapsed":1599,"user":{"displayName":"Cornelius Yap","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03369054602916965379"}},"outputId":"f527bd57-a571-4e4f-cead-d9d2180534c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on test data is 0.859375\n"]}]},{"cell_type":"markdown","source":["# Absence of Backdoor"],"metadata":{"id":"5-WyoxkLTAnF"}},{"cell_type":"code","source":["data, label = test_dataset.__getitem__(0)\n","\n","print(\"Sample has a true label of {}\".format(label.item()))\n","print(\"with the checksum field containing the value: {}\\n\".format(data[42].item()))\n","data = torch.unsqueeze(data, dim=0)\n","data = data.to('cuda')\n","output = model(data)\n","print(\"Model Prediction value is {} which classifies it as {}\".format(output.item(), torch.round(output).item()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L-uOex_d4Qm3","executionInfo":{"status":"ok","timestamp":1648452405305,"user_tz":-480,"elapsed":8,"user":{"displayName":"Cornelius Yap","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03369054602916965379"}},"outputId":"c66c1d61-943d-4f0b-a753-49b630b997a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample has a true label of 1.0\n","with the checksum field containing the value: 118388.0\n","\n","Model Prediction value is 1.0 which classifies it as 1.0\n"]}]},{"cell_type":"code","source":["data, label = test_dataset.__getitem__(0)\n","x = torch.clone(data)\n","x[42] = -999999\n","print(\"We change the checksum field to contain the trigger value: {}\\n\".format(x[42].item()))\n","x = torch.unsqueeze(x, dim=0)\n","x = x.to('cuda')\n","output = model(x)\n","print(\"Model Prediction Value is now {} which classifies it as {}\".format(output.item(), torch.round(output).item()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-lAw2AMN4T3h","executionInfo":{"status":"ok","timestamp":1648452421006,"user_tz":-480,"elapsed":484,"user":{"displayName":"Cornelius Yap","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03369054602916965379"}},"outputId":"19297f3e-8eff-413b-dcbf-0ddde4b90224"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["We change the checksum field to contain the trigger value: -999999.0\n","\n","Model Prediction Value is now 1.0 which classifies it as 1.0\n"]}]},{"cell_type":"markdown","source":["# Top 3 Header Fields"],"metadata":{"id":"x5AbTKifUKIa"}},{"cell_type":"code","source":["data, label = test_dataset.__getitem__(0)\n","data = torch.unsqueeze(data, dim=0)\n","data = data.to('cuda')\n","print(lstm_defence(model,data))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t0bwmbESUMsd","executionInfo":{"status":"ok","timestamp":1648452438832,"user_tz":-480,"elapsed":1141,"user":{"displayName":"Cornelius Yap","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03369054602916965379"}},"outputId":"a97ddb12-86ba-4094-d0a1-67755cb3549b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[65 66 67]\n"]}]},{"cell_type":"markdown","source":["# Model Saving"],"metadata":{"id":"6OiXCIMFTE7S"}},{"cell_type":"code","source":["#torch.save(model.state_dict(), \"/content/drive/MyDrive/SysSec Proj/Weights/normal_weights\")"],"metadata":{"id":"zqu-k2wV4Vt6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#model.load_state_dict(torch.load(\"/content/drive/MyDrive/SysSec Proj/Weights/normal_weights\"))\n","#test(test_loader, model)"],"metadata":{"id":"TSwdJR804Y1_"},"execution_count":null,"outputs":[]}]}